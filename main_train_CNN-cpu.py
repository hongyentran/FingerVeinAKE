"""
This code is generated by Ridvan Salih KUZU @UNIROMA3
LAST EDITED:  02.03.2020
ABOUT SCRIPT:
It is a main script for training a verification system for a given database.
It considers CUSTOM PENALTY functions

BEST EER RESULTS ARE TAKEN ON 1 GPU when embedding_size=1024, batch_size=32:
EER=0.00000 - Densenet-161 --weight-decay 0.0050 --learning-rate 0.010  --margin 0.6 --scale 16 --type aamp --embedding-size 1024 --outdir modeldir/00/27/
EER=0.00023 - Densenet-161 --weight-decay 0.0100 --learning-rate 0.010  --margin 0.6 --scale 16 --type aamp --embedding-size 1024 --outdir modeldir/00/03/
"""
import math
import secrets
import string
import time

import numpy as np
import argparse
import torch
from torch.optim import lr_scheduler
from utils import FullPairComparer,AverageMeter, evaluate, plot_roc, plot_DET_with_EER, plot_density,accuracy, crypto_primitives
from models import DenseNet161_Modified as net
from benchmark_verification import get_dataloader
from feature_extraction import feature_extractor
from models import DenseNet161_Modified as densenet
from benchmark_verification import get_instance_data_loader
from losses import *
import shutil
from torch.nn import CrossEntropyLoss
import pandas as pd
import os
from itertools import chain
import csv
import pickle
import hashlib, ecdsa, hmac
# from py_ecc import optimized_bls12_381 as bls381
from py_ecc import bls12_381 as bls381
import random
import struct
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
import matplotlib.pyplot as plt

NUMBER_OF_BYTES = 4

parser = argparse.ArgumentParser(description='Vein Verification')

parser.add_argument('--start-epoch', default=1, type=int, metavar='SE',
                    help='start epoch (default: 0)')
parser.add_argument('--num-epochs', default=60, type=int, metavar='NE',
                    help='number of epochs to train (default: 90)')
parser.add_argument('--num-classes', default=220, type=int, metavar='NC',
                    help='number of clases (default: 318)')
parser.add_argument('--embedding-size', default=1024, type=int, metavar='ES',
                    help='embedding size (default: 128)')
parser.add_argument('--batch-size', default=32, type=int, metavar='BS',
                    help='batch size (default: 128)')
parser.add_argument('--num-workers', default=8, type=int, metavar='NW',
                    help='number of workers (default: 8)')
parser.add_argument('--learning-rate', default=0.01, type=float, metavar='LR',
                    help='learning rate (default: 0.01)') #seems best when SWG off
parser.add_argument('--weight-decay', default=0.0100, type=float, metavar='WD',
                    help='weight decay (default: 0.01)') #seems best when SWG off
parser.add_argument('--scale-rate', default=16, type=float, metavar='SC',
                    help='scale rate (default: 0.001)')
parser.add_argument('--margin', default=0.6, type=float, metavar='MG',
                    help='margin (default: 0.5)')
parser.add_argument('--database-dir', default='Data_SDUMLA/Database', type=str,
                    help='path to the database root directory')
parser.add_argument('--train-dir', default='Data_SDUMLA/CSVFiles/sdumla_train_dis.csv', type=str,
                    help='path to train root dir')
parser.add_argument('--valid-dir', default='Data_SDUMLA/CSVFiles/sdumla_val_dis.csv', type=str,
                    help='path to valid root dir')
parser.add_argument('--test-dir', default='Data_SDUMLA/CSVFiles/sdumla_test_pairs_dis.csv', type=str,
                    help='path to test root dir')
parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',
                    help='evaluate model on test set')
parser.add_argument('--type', default='aamp', type=str, metavar='MG',
                    help='type (default: aamp)')
parser.add_argument('--outdir', default='modeldir/sdumla', type=str,
                    help='Out Directory (default: model)')
parser.add_argument('--logdir', default='Data_SDUMLA/CSVFiles/ver_logs.csv', type=str,
                    help='path to log dir')

args = parser.parse_args()
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')


def getBioX(model, image_path):
    # model_dir = "modeldir/sdumla"
    # data_loader = get_instance_data_loader("feature_extraction/middle_5.bmp")
    data_loader = get_instance_data_loader(image_path)
    # print('Time for image loader ', time.perf_counter() - t)
    # model = densenet(embedding_size=args.embedding_size, pretrained=True)
    # model = torch.nn.DataParallel(model, device_ids=[0]).cuda()
    # model = torch.nn.DataParallel(model, device_ids=[0]).cpu()
    # checkpoint = torch.load(model_dir + '/model_best.pth.tar', map_location=torch.device("cpu"))
    # model.load_state_dict(checkpoint['state_dict'])
    with torch.set_grad_enabled(False):
        model.eval()
        # for index, (data, file_name) in enumerate(data_loader):
        image = data_loader.dataset[0][0]
        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])
        #     x = model(data,False)
        x = model(image, False)
    return x.data.cpu().numpy().T
def generate_B_tri(n, d, B):
    B[0, 0] = d  # initialize 0-th row
    for k in range(1, n):  # generate k-th row
        B[k] = B[k - 1]
        sum = 0;
        for i in range(k - 1):
            sum += B[k, i] ** 2
        if k == 1:
            B[k, k - 1] = d ** 2 / 2 / B[k - 1, k - 1]
        else:
            B[k, k - 1] = (d ** 2 / 2 - sum) / B[k - 1, k - 1]
        sum += B[k, k - 1] ** 2
        # print(sum)
        B[k, k] = math.sqrt(d ** 2 - sum)
def is_B_tri(B, d, n):
    for i in range(n):
        if (np.abs(np.sum(np.square(B[i])) - d ** 2) > 0.00001):
            print("b[i] - d**2 = ", np.abs(np.sum(np.square(B[i])) - d ** 2))
            return False

    for i in range(n):
        for j in range(n):
            if i != j:
                if (np.abs(np.sum(np.multiply(B[i], B[j])) - d ** 2 / 2) > 0.00001):
                    print("b[i]*b[j] - d**2/2 = ", np.abs(np.sum(np.multiply(B[i], B[j])) - d ** 2 / 2))
                    return False
def closest_vector(n, B, xB):  # xB is in the representation of x in L: B*xB = x = B*B^{-1}*x
    # # Convert x to the repsentation in L
    int_part = [(math.floor(xB[i])) for i in range(len(xB))]
    xB = xB - int_part
    # print('xB \in [0,1) =',xB)
    # sort x
    sx_ = pd.Series(index=range(n), data=np.array(xB))
    sx = sx_.sort_values(ascending=True)
    # print('sx=', sx)

    # Find the set of closet vector candidates y_0, y_1, ... y_n
    # t_y = time.perf_counter()
    y = np.zeros((n + 1, n), dtype=int)
    y[0] = np.ones(n, dtype=int)
    y[n] = np.zeros(n, dtype=int)
    for k in range(1, n):
        y[k] = y[k-1]
        y[k, sx.index[k-1]] = 0
    # print(y)
    # print('time to find the set y: = ', time.perf_counter() - t_y)

    # Compute the distance(x, y_i). Obtain the y_mini with smallest distance.
    # t_dist = time.perf_counter()
    min = 100000000
    mini = 0
    for i in range(n + 1):
        # print(y[i])
        e_dist = np.linalg.norm(np.matmul(B, xB - y[i]), ord=2)
        # e_dist = np.linalg.norm(xB - y[i], ord=2)
        # print(e_dist)
        if min > e_dist:
            min = e_dist
            mini = i
    # print('time to compute the distance of n+1: = ', time.perf_counter() - t_dist)

    return (y[mini] + int_part)

# def find_d(fuzzy_idx, f1, f2, n):
#     global best_EER
#     global best_d
#     global count
#     global d
#     count = count + 1
#     print('*'*30)
#     print('count = ', count)
#     print('d = ', d)
#     FA = 0
#     FR = 0
#     B = np.zeros((n, n))
#     generate_B_tri(n, d, B)
#     for i in fuzzy_idx[0]: # impostor pairs
#         cv = closest_vector(n, B, np.matmul(np.linalg.inv(B), f1[i] - f2[i]))
#         if np.count_nonzero(cv) == 0:
#             FA = FA + 1
#     for i in fuzzy_idx[1]: # genuine pairs
#         cv = closest_vector(n, B, np.matmul(np.linalg.inv(B), f1[i] - f2[i]))
#         if np.count_nonzero(cv) > 0:
#             FR = FR + 1
#     EER = np.mean(FA + FR) / 2
#     print('FA = ', FA)
#     print('FR = ', FR)
#     print('EER = ', EER)
#     if EER < best_EER:
#         best_EER = EER
#         best_d = d
#     if count == 100:
#         return
#
#     if FA >= FR:
#         d = d - d/2
#         find_d(fuzzy_idx, f1, f2, n)
#     elif FA < FR:
#         d = d + d/2
#         find_d(fuzzy_idx, f1, f2, n)



# def find_d(s0, s, d, step, i):
#     # print('new find_d')
#     # print(d)
#     # print(step)
#     if (abs(step[i]) < 0.00001):
#         return
#     n = len(s0)
#     B = np.zeros((n, n))
#     generate_B_tri(n, d[i], B)
#     cv = closest_vector(n, B, np.matmul(np.linalg.inv(B), s0 - s))
#     if (np.count_nonzero(cv) == 0):
#         step[i] = -abs(step[i]) / 2  # step < 0
#         d[i] = d[i] + step[i]
#         find_d(s0, s, d, step, i)
#     else:
#         step[i] = abs(step[i])  # step > 0
#         d[i] = d[i] + step[i]
#         find_d(s0, s, d, step, i)

# def main():
#
#     args.evaluate=True
#     if not os.path.exists(args.outdir):
#         os.makedirs(args.outdir)
#
#     device = torch.device("cpu")
#
#     criterion = CrossEntropyLoss()
#     if args.type == 'norm':
#         loss_metric = NormSoftmax(args.embedding_size, args.num_classes, args.scale_rate)
#     elif args.type == 'aamp':
#         loss_metric = ArcMarginProduct(args.embedding_size, args.num_classes, s=args.scale_rate, m=args.margin)
#     elif args.type == 'lmcp':
#         loss_metric = AddMarginProduct(args.embedding_size, args.num_classes, s=args.scale_rate, m=args.margin)
#     elif args.type == 'sphere':
#         loss_metric = SphereProduct(args.embedding_size, args.num_classes, m=int(args.margin))
#     elif args.type == 'lgm':
#         loss_metric = CovFixLGM(args.embedding_size, args.num_classes, args.margin)
#     elif args.type == 'lgm2':
#         loss_metric = LGMLoss(args.embedding_size, args.num_classes, args.margin)
#     elif args.type == 'none':
#         loss_metric = None
#
#     if loss_metric is not None:
#         model = net(embedding_size=args.embedding_size, class_size=args.num_classes, only_embeddings=True,pretrained=True)
#         to_be_optimized=chain(model.parameters(), loss_metric.parameters())
#     else:
#         model = net(embedding_size=args.embedding_size, class_size=args.num_classes, only_embeddings=False,pretrained=True)
#         to_be_optimized = model.parameters()
#
#     model = torch.nn.DataParallel(model)
#     optimizer = torch.optim.SGD(to_be_optimized,
#                                 lr=args.learning_rate,
#                                 momentum=0.9,
#                                 weight_decay=args.weight_decay)
#
#
#     #scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
#     scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, cooldown=2,verbose=True)
#
#     # if args.start_epoch != 0:
#     #     checkpoint = torch.load(args.outdir+'/model_checkpoint.pth.tar')
#     #     args.start_epoch = checkpoint['epoch']
#     #     model.load_state_dict(checkpoint['state_dict'])
#     #     optimizer.load_state_dict(checkpoint['optimizer'])
#     #     if loss_metric is not None:
#     #         loss_metric.load_state_dict(checkpoint['metric'])
#     if args.evaluate: # test
#         checkpoint = torch.load(args.outdir+'/model_best.pth.tar', map_location=torch.device("cpu"))
#         args.start_epoch = checkpoint['epoch']
#         model.load_state_dict(checkpoint['state_dict'])
#         optimizer.load_state_dict(checkpoint['optimizer'])
#         #if loss_metric is not None:
#         #    loss_metric.load_state_dict(checkpoint['metric'])
#         data_loaders = get_dataloader(args.database_dir,args.train_dir, args.valid_dir, args.test_dir,
#                                       args.batch_size, args.num_workers)
#         # data_loaders = get_dataloader(args.database_dir, None, None, args.test_dir,
#         #                               args.batch_size, args.num_workers)
#         test(model,data_loaders['test'],'00',is_graph=True)
#
#     else: # train
#         for epoch in range(args.start_epoch, args.num_epochs + args.start_epoch):
#             print(80 * '=')
#             print('Epoch [{}/{}]'.format(epoch, args.num_epochs + args.start_epoch - 1))
#
#             data_loaders = get_dataloader(args.database_dir,args.train_dir, args.valid_dir,args.test_dir,
#                                       args.batch_size, args.num_workers)
#
#             train(model, optimizer, epoch, data_loaders['train'],criterion,loss_metric)
#             is_best, acc, loss=validate(model, optimizer, epoch, data_loaders['valid'], criterion,loss_metric)
#             scheduler.step(loss)
#             if is_best and acc>100:
#                 test(model,data_loaders['test'],epoch,is_graph=True)
#
#         print(80 * '=')
#
#         ## MODEL EVALUATION LOGGING ##
#         data_loaders = get_dataloader(args.database_dir, args.train_dir, args.valid_dir, args.test_dir,
#                                   args.batch_size, args.num_workers)
#         checkpoint = torch.load(args.outdir + '/model_best.pth.tar')
#         model.load_state_dict(checkpoint['state_dict'])
#         EER = test(model, data_loaders['test'], epoch,is_graph=False)
#
#         header = ['weight_decay', 'learning_rate', 'scale', 'margin','type', 'batch_size', 'embedding_size', 'EER', 'out_dir' ]
#         info = [args.weight_decay, args.learning_rate, args.scale_rate, args.margin, args.type, args.batch_size,args.embedding_size, EER, args.outdir]
#
#         if not os.path.exists(args.logdir):
#             with open(args.logdir, 'w') as file:
#                 logger = csv.writer(file)
#                 logger.writerow(header)
#                 logger.writerow(info)
#         else:
#             with open(args.logdir, 'a') as file:
#                 logger = csv.writer(file)
#                 logger.writerow(info)
#
# def test(model, dataloader,epoch,is_graph=False):
#     global best_test
#     labels, distances = [], []
#     features1, features2 = [], []
#     print(model)
#     with torch.set_grad_enabled(False):
#         comparer = FullPairComparer()
#         model.eval()
#         count = 0
#         for batch_idx, (data1, data2, target) in enumerate(dataloader):
#             print(count)
#             dist = []
#
#             output1 = model(data1,False)
#             output2 = model(data2,False)
#             dist = comparer(output1, output2) #TODO: sign - torch.sign()
#             #dist = comparer(torch.sign(F.relu(output1)), torch.sign(F.relu(output2)))  # TODO: sign - torch.sign()
#             features1.append(output1.data.cpu().numpy())
#             features2.append(output2.data.cpu().numpy())
#             distances.append(dist.data.cpu().numpy())
#             labels.append(target.data.cpu().numpy())
#             count = count + 1
#             if batch_idx % 50 == 0:
#                 print('Batch-Index -{}'.format(str(batch_idx)))
#
#     labels = np.array([sublabel for label in labels for sublabel in label])
#     distances = np.array([subdist for dist in distances for subdist in dist])
#     tpr, fpr, fnr, fpr_optimum, fnr_optimum, accuracy, threshold = evaluate(distances, labels)
#
#
#     EER = np.mean(fpr_optimum + fnr_optimum) / 2
#     print('TEST - Accuracy           = {:.12f}'.format(accuracy))
#     print('TEST - EER                = {:.12f}'.format(EER))
#     is_best = EER <= best_test
#     best_test = min(EER, best_test)
#
#     if is_best and is_graph:
#         plot_roc(fpr, tpr, figure_name=args.outdir + '/Test_ROC-{}.png'.format(epoch))
#         plot_DET_with_EER(fpr, fnr, fpr_optimum, fnr_optimum,
#                           figure_name=args.outdir + '/Test_DET-{}.png'.format(epoch))
#         plot_density(distances, labels, figure_name=args.outdir + '/Test_DENSITY-{}.png'.format(epoch))
#         df_results = pd.DataFrame({'distances': distances.transpose(), 'labels': labels.transpose()})
#         df_results.to_csv(args.outdir + "/test_outputs.csv", index=False)
#         df_feature_pairs = pd.DataFrame({'f1': features1, 'f2': features2})
#         with open(args.outdir + '/test_feature_pairs.dat', 'wb') as f:
#             pickle.dump(df_feature_pairs, f)
#             print("Dumping feature pairs done!")
#         with open(args.outdir + '/test_outputs.dat', 'wb') as f:
#             pickle.dump(df_results, f)
#             print("Dumping outputs done!")
#
#         if args.evaluate is False:
#             shutil.copyfile(args.outdir + '/model_best.pth.tar', args.outdir + '/test_model_best.pth.tar')
#
#     return EER
#
# def train(model, optimizer, epoch, dataloader, criterion, metric):
#     with torch.set_grad_enabled(True):
#         losses = AverageMeter()
#         top1 = AverageMeter()
#         model.train()
#
#         for batch_idx, (data, target,_) in enumerate(dataloader):
#             optimizer.zero_grad()
#
#             outputs = model(data)
#             if metric is not None:
#                 outputs = metric(outputs, target)
#             loss = criterion(outputs,target)
#
#             prec1, prec5 = accuracy(outputs, target, topk=(1, 5))
#             losses.update(loss.item(), data.size(0))
#             top1.update(prec1[0], data.size(0))
#             loss.backward()
#             optimizer.step()
#             if batch_idx % 25 == 0:
#                 print('Step-{} Prec@1 {top1.avg:.5f} loss@1 - {loss.avg:.5f}'.format(batch_idx, top1=top1, loss=losses))
#
#         print('*TRAIN Prec@1 {top1.avg:.5f} - loss@1 {loss.avg:.5f}'.format(top1=top1, loss=losses))
#
# def validate(model, optimizer, epoch, dataloader, criterion, metric):
#     global best_losss
#     with torch.set_grad_enabled(False):
#         losses = AverageMeter()
#         top1 = AverageMeter()
#
#         model.eval()
#
#         for batch_idx, (data, target,_) in enumerate(dataloader):
#
#             outputs = model(data)
#             if metric is not None:
#                 outputs = metric(outputs, target)
#             loss = criterion(outputs, target)
#             prec1, prec5 = accuracy(outputs, target, topk=(1, 5))
#             losses.update(loss.item(), data.size(0))
#             top1.update(prec1[0], data.size(0))
#             if batch_idx % 5 == 0:
#                 print('Step-{} Prec@1 {top1.avg:.5f} loss@1 - {loss.avg:.5f}'.format(batch_idx,top1=top1,loss=losses))
#
#         print('*VALID Prec@1 {top1.avg:.5f} - loss@1 {loss.avg:.5f}'.format(top1=top1,loss=losses))
#         is_best = losses.avg <= best_losss
#         best_losss = min(losses.avg, best_losss)
#         if metric is not None:
#             torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict(), 'metric': metric.state_dict(),
#                         'optimizer': optimizer.state_dict()}, args.outdir + '/model_checkpoint.pth.tar')
#         else:
#             torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict(),
#                         'optimizer': optimizer.state_dict()}, args.outdir + '/model_checkpoint.pth.tar')
#
#         if is_best:
#             shutil.copyfile(args.outdir+'/model_checkpoint.pth.tar', args.outdir+'/model_best.pth.tar')
#
#         return is_best, top1.avg, losses.avg
#
def verify():
    if (os.path.exists(args.outdir + '/test_feature_pairs.dat')):
        with open(args.outdir + '/test_feature_pairs.dat', 'rb') as f:
            feature_pairs = pickle.load(f)
        print("Loading features done!")
    # print(len(feature_pairs['f1']))
    f1 = np.array([feature for feature_batch in feature_pairs['f1'] for feature in feature_batch])
    f2 = np.array([feature for feature_batch in feature_pairs['f2'] for feature in feature_batch])


    if (os.path.exists(args.outdir + '/test_outputs.dat')):
        with open(args.outdir + '/test_outputs.dat', 'rb') as f:
            outputs = pickle.load(f)
        print("Loading outputs done!")
    print('Max distance: ', np.max(outputs['distances']))
    print('Min distance: ', np.min(outputs['distances']))
    # for i in range(10):
    #     print('Euclidian Distance: ', np.linalg.norm(f1[i] - f2[i], ord=2))
    # print(outputs['distances'][0:10])

    tpr, fpr, fnr, fpr_optimum, fnr_optimum, accuracy, threshold = evaluate(outputs['distances'], outputs['labels'])

    EER = np.mean(fpr_optimum + fnr_optimum) / 2
    print('TEST - Accuracy           = {:.12f}'.format(accuracy))
    print('TEST - EER                = {:.12f}'.format(EER))
    print('TEST - fpr_optimum        = {:.12f}'.format(fpr_optimum))
    print('TEST - fnr_optimum        = {:.12f}'.format(fnr_optimum))
    print('TEST - Threshold          = {:.12f}'.format(threshold))
    plot_roc(fpr, tpr, figure_name=args.outdir + '/Test_ROC-best.png')
    plot_DET_with_EER(fpr, fnr, fpr_optimum, fnr_optimum,
                      figure_name=args.outdir + '/Test_DET-best.png')
    plot_density(outputs['distances'].to_numpy(), outputs['labels'].to_numpy(), figure_name=args.outdir + '/Test_DENSITY-best.png')



    n = len(f1[0])
    num_test_pairs = len(f1)
    num_genuine_pairs = np.count_nonzero(outputs['labels']) # = FN_TP
    num_impostor_pairs = num_test_pairs - num_genuine_pairs # = FP_TN
    # print('n = ', n)
    print('num_test_pairs = ', num_test_pairs)
    print('num_genuine_pairs = ', num_genuine_pairs)
    print('num_impostor_samples = ', num_impostor_pairs)

    # min_dis_impostor = 100
    # max_dis_genuine = -1
    # imax = 0,
    # imin = 0
    # for i in range(num_test_pairs):
    #     if outputs['labels'][i] == 1:
    #         if max_dis_genuine < outputs['distances'][i]:
    #             max_dis_genuine = outputs['distances'][i]
    #             imax = i
    #     else:
    #         if min_dis_impostor > outputs['distances'][i]:
    #             min_dis_impostor = outputs['distances'][i]
    #             imin = i
    # print('max_dis_genuine = {}'.format(max_dis_genuine), ', imax = {}'.format(imax))
    # print('min_dis_impostor = {}'.format(min_dis_impostor), ', imin = {}'.format(imin))



    # Find d

    dist_impostor_pairs = pd.Series(index=range(num_impostor_pairs), data=outputs['distances'][0:num_impostor_pairs])
    dist_impostor_pairs = dist_impostor_pairs.sort_values(ascending=True)
    print(dist_impostor_pairs[0:10])
    # print(dis_impostor_pairs.index[0:10])


    dist_genuine_pairs = pd.Series(index=range(num_impostor_pairs, num_test_pairs), data=outputs['distances'][num_impostor_pairs: num_test_pairs])
    dist_genuine_pairs = dist_genuine_pairs.sort_values(ascending=False)
    print(dist_genuine_pairs[0:10])

    # print(dis_genuine_pairs.index[0:10])
    fuzzy_idx = []
    fuzzy_idx.append(dist_impostor_pairs.index[0:10].tolist())
    fuzzy_idx.append(dist_genuine_pairs.index[0:10].tolist())
    # print(fuzzy_idx)

    # Find d
    # global d
    # d = threshold
    # find_d(fuzzy_idx, f1, f2, n)

    # Create B lattice
    # print('best_d = ', best_d)
    # best_d = 0.2636031368701879 (Done) / 0.25709116979964936 (Test) (FA = 5, FR = 0, EER = 2.5)
    # best_d = 0.2536308174038483 # (FA = 5, FR = 2, EER =  3.5) Test
    # best_d = 0.25021704007599865 (FA = 4, FR = 3, EER = 3.5)
    # best_d = 0.24684921093285084 (Done) / 0.2473652033044949 Test (FA = 4, FR = 4, EER = 4)
    # best_d = 0.24075112745755917 / 0.2440357588332609 (FA = 3, FR = 4, EER = 3.5) Test
    # best_d = 0.23751070600963936 (FA = 2, FR = 5, EER = 3.5)
    # best_d = 0.2348036890741885 (FA =  1, FR =  7, EER =  4.0)
    # best_d = 0.23164331795500936 / 0.22544961547008735 (FA = 0, FR = 8, EER = 4)
    # best_d = 0.22544961547008735
    #
    # B = np.zeros((n, n))
    # B_inv = np.zeros((n, n))
    # generate_B_tri(n, best_d, B)
    # B_inv = np.linalg.inv(B)
    #
    # # Calculate FAR, FRR, and Accuracy
    # FR = []
    # FA = []
    # ACC = 0
    #
    # for i in range(num_test_pairs):
    #     if (i%100 == 0):
    #         print('i = ', i)
    #     cv = closest_vector(n, B, np.matmul(B_inv, f1[i] - f2[i]))
    #     if (np.count_nonzero(cv) > 0) and (outputs['labels'][i] == 1):
    #         FR.append(i)
    #         print('FR: ', i)
    #     if (np.count_nonzero(cv) == 0) and (outputs['labels'][i] == 0):
    #         FA.append(i)
    #         print('FA: ', i)
    #     else:
    #         ACC = ACC + 1
    # print('FA = ', FA)
    # print('FR = ', FR)
    # print('FAR = ', 100 * len(FA) / num_impostor_pairs)
    # print('FRR = ', 100 * len(FR) / num_genuine_pairs)
    # print('ACC = ', 100 * ACC / num_test_pairs)

    FR_idx = [24453, 24494, 24513, 24692, 25118, 25130, 25131, 25143, 25149, 25174, 25340, 25529,
              25535, 26627, 26924, 27465, 27469, 27470, 27475, 27481, 27482, 27488, 27494, 27495,
              27576, 27673, 27674, 27679, 27705, 28182, 28207, 28236, 28237]
    FA_idx = [406, 718, 1188, 2500, 2866, 3754, 3870, 5461, 6311, 8241, 8574, 10035, 10225, 12583,
              13294, 14869, 16194, 16717, 18595, 19360, 20559, 22107, 22610]
    d1 = 0.2512441681723816
    d2 = 0.2546719588808034
    d = [None] * 100
    far = [None] * 100
    frr = [None] * 100
    acc = [None] * 100
    FAs = [None] * 100
    FRs = [None] * 100

    estimate_d(FA_idx, FR_idx, f1, f2, n, d, d1, d2, far, frr, acc, FAs, FRs, num_impostor_pairs, num_genuine_pairs)

    outputs = {"d": d, "FAR": far, "FRR": frr, "ACC": acc, "FAs": FAs, "FRs": FRs}
    d_fa_fr = pd.DataFrame.from_dict(outputs)
    with open(args.outdir + '/test_d_FA_FR.dat', 'wb') as f:
        pickle.dump(d_fa_fr, f)
        print("Dumping d_fa_fr done!")
    with open(args.outdir + '/test_d_FA_FR.dat', 'rb') as f:
        d_fa_fr = pickle.load(f)
        print("Loading d_fa_fr done!")
# def setup():
#     global f1
#     global f2
#     global n
#     global d
#     global B
#     global num_test_pairs
#     global model
#     global image
#     global private_key
#     global public_key
#
#     if (os.path.exists(args.outdir + '/test_feature_pairs.dat')):
#         with open(args.outdir + '/test_feature_pairs.dat', 'rb') as f:
#             feature_pairs = pickle.load(f)
#         print("Loading features done!")
#     # print(len(feature_pairs['f1']))
#     f1 = np.array([feature for feature_batch in feature_pairs['f1'] for feature in feature_batch])
#     f2 = np.array([feature for feature_batch in feature_pairs['f2'] for feature in feature_batch])
#
#
#     if (os.path.exists(args.outdir + '/test_outputs.dat')):
#         with open(args.outdir + '/test_outputs.dat', 'rb') as f:
#             outputs = pickle.load(f)
#         print("Loading outputs done!")
#     print('Max distance: ', np.max(outputs['distances']))
#     print('Min distance: ', np.min(outputs['distances']))
#
#     n = len(f1[0])
#     num_test_pairs = len(f1)
#     num_genuine_pairs = np.count_nonzero(outputs['labels'])  # = FN_TP
#     num_impostor_pairs = num_test_pairs - num_genuine_pairs  # = FP_TN
#     # print('n = ', n)
#     print('num_test_pairs = ', num_test_pairs)
#     print('num_genuine_pairs = ', num_genuine_pairs)
#     print('num_impostor_samples = ', num_impostor_pairs)
#
#     dist_impostor_pairs = pd.Series(index=range(num_impostor_pairs), data=outputs['distances'][0:num_impostor_pairs])
#     dist_impostor_pairs = dist_impostor_pairs.sort_values(ascending=True)
#     # print(dist_impostor_pairs[0:10])
#     # print(dis_impostor_pairs.index[0:10])
#
#     dist_genuine_pairs = pd.Series(index=range(num_impostor_pairs, num_test_pairs),
#                                    data=outputs['distances'][num_impostor_pairs: num_test_pairs])
#     dist_genuine_pairs = dist_genuine_pairs.sort_values(ascending=False)
#     # print(dist_genuine_pairs[0:10])
#
#     # Create B
#     d = 0.2540647565201628
#     B = np.zeros((n, n))
#     generate_B_tri(n, d, B)
#
#     # Set up for signature
#     private_key = ecdsa.SigningKey.generate(curve=ecdsa.SECP256k1)
#     public_key = private_key.get_verifying_key()
#
#     # Loading model and reading image
#     image_dir = "feature_extraction/middle_5.bmp"
#     model_dir = "modeldir/sdumla"
#     data_loader = get_instance_data_loader(image_dir)
#     # print('Time for image loader ', time.perf_counter() - t)
#     model = densenet(embedding_size=args.embedding_size, pretrained=True)
#     # model = torch.nn.DataParallel(model, device_ids=[0]).cuda()
#     model = torch.nn.DataParallel(model, device_ids=[0]).cpu()
#     checkpoint = torch.load(model_dir + '/model_best.pth.tar', map_location=torch.device("cpu"))
#     model.load_state_dict(checkpoint['state_dict'])
#
#     with torch.set_grad_enabled(False):
#         model.eval()
#         for index, (data, file_name) in enumerate(data_loader):
#             # image = data_loader.dataset[0][0]
#             image = data
#             # output = model(data,False)

def p2b(ep):
    return (int(ep[0]).to_bytes(48, byteorder='little') +
            int(ep[1]).to_bytes(48, byteorder='little'))
def i2b(array, size):
    # # Convert each integer to 8 bytes (equivalent to float-type in python) and concatenate
    return b"".join(int(x).to_bytes(size, byteorder="little", signed=True) for x in array)
    # byte_array = b"".join(struct.pack('q', f) for f in array)
    # return byte_array
def b2i(bytes, size):
    # Convert 8 bytes into each integer
    return [int.from_bytes(bytes[i:i+size], byteorder="little", signed=True) for i in range(0, len(bytes), size)]
    # int64_array = [struct.unpack('q', bytes[i:i + size])[0] for i in range(0, len(bytes), size)]
    # return int64_array
def f2b(array):
    # # Convert each integer to 8 bytes (equivalent to float-type in python) and concatenate
    # return b"".join(int(x).to_bytes(size, byteorder="little", signed=True) for x in array)
    byte_array = b"".join(struct.pack('d', f) for f in array)
    return byte_array
def b2f(bytes):
    # Convert 8 bytes into each integer
    float64_array = [struct.unpack('d', bytes[i:i + 8])[0] for i in range(0, len(bytes), 8)]
    return float64_array
def enc(key, plaintext):
    """
        AES-CBC encryption of an array of numbers using key
        key: key 32 bytes
        plaintext: byte array
        """
    # # Convert integers to bytes
    # plaintext = i2b(array, size)

    # Generate a random IV (Initialization Vector)
    iv = os.urandom(16)

    # Create an AES cipher object with CBC mode
    cipher = Cipher(algorithms.AES(key), modes.CFB(iv), backend=default_backend())

    # Create an encryptor object
    encryptor = cipher.encryptor()

    # Encrypt the data
    ciphertext = encryptor.update(plaintext) + encryptor.finalize()
    # return ([int.from_bytes(ciphertext[i:i+32], byteorder="little", signed=True) for i in range(0, len(ciphertext), 32)], iv)
    return iv + ciphertext
def dec(key, ciphertext):
    """
        AES-CBC decryption of ciphertext, return an array of integers s.t. len(array) = (len(ciphertext) - 16)/size
        key: key to encrypt
        ciphertext: byte array, in which the first 16 bytes are iv
        """
    # Extract the IV from the ciphertext
    iv = ciphertext[:16]

    # Create an AES cipher object with CBC mode
    cipher = Cipher(algorithms.AES(key), modes.CFB(iv), backend=default_backend())

    # Create a decryptor object
    decryptor = cipher.decryptor()

    # Decrypt the data
    decrypted_bytes = decryptor.update(ciphertext[16:]) + decryptor.finalize()

    # # Convert decrypted bytes to integers
    # decrypted_int_array = [int.from_bytes(decrypted_bytes[i:i+32], byteorder="little", signed=True) for i in range(0, len(decrypted_bytes), 32)]

    return decrypted_bytes
class MFFE:
    global c_reg, e_reg
    def __init__(self):
        self.pp = None
        self.UH = None
    def setup(self, n, d):
        B = np.zeros((n, n))
        generate_B_tri(n, d, B)
        uh = [random.randint(0, bls381.curve_order - 1) for i in range(n+1)] # (n+1) randoms in Z_q
        self.pp = {'n': n, 'B': B, 'g': bls381.G1}
        self.UH = uh
    def gen(self, x, ga):
        global c_reg, e_reg, plaintext_reg, ciphertext_reg
        # c = \floor(B^{-1}x) || (H(g^a) % q)
        hash_ga = int.from_bytes(hashlib.sha3_256(p2b(ga)).digest(), byteorder='little') % bls381.curve_order
        c = np.append(np.floor(np.matmul(np.linalg.inv(self.pp['B']), x)).astype(int), hash_ga) # float64
        c_reg = c
        beta = np.inner(self.UH, c) % bls381.curve_order
        # print('beta in Gen = ', beta)
        r = random.randint(0, bls381.curve_order - 1)
        w = bls381.multiply(bls381.G1, r)
        k = hashlib.sha3_256(p2b(bls381.multiply(ga, r))).digest()
        plaintext = i2b(c[0:self.pp['n']], size=NUMBER_OF_BYTES)
        plaintext_reg = plaintext
        e = enc(k, plaintext)
        ciphertext_reg = e
        # e2 = enc(k, [c[self.pp['n']]], 32)
        # convert ciphertext bytes to ciphertext int64
        e_reg = b2i(e[16:], size=NUMBER_OF_BYTES)
        # print('encryption in Gen = ', e)
        iv = e[:16]
        # delta = np.append(np.matmul(np.linalg.inv(self.pp['B']), x), hash_ga) - np.append(b2i(e1[16:], 8), b2i(e2[16:], 32))
        e2f = [float(i) for i in e_reg] # expand to float 8 bytes
        delta = np.matmul(np.linalg.inv(self.pp['B']), x).reshape((self.pp['n'])) - e2f
        # delta = b2i(e[16:], size=NUMBER_OF_BYTES)
        return (beta, delta, w, iv)
    def rep(self, x, a, delta, w, iv):
        global c_rep, e_rep, ciphertext_rep, plaintext_rep
        k = hashlib.sha3_256(p2b(bls381.multiply(w, a))).digest()
        ga = bls381.multiply(self.pp['g'], a)
        hash_ga = int.from_bytes(hashlib.sha3_256(p2b(ga)).digest(), byteorder='little') % bls381.curve_order
        diff = np.matmul(np.linalg.inv(self.pp['B']), x).reshape((self.pp['n'],)) - delta[0:self.pp['n']]
        # diff = np.array(delta[0:self.pp['n']])
        # tmp = time.perf_counter()
        cv = closest_vector(self.pp['n'], self.pp['B'], diff)
        # print('client time to find CV: ', time.perf_counter() - tmp)
        e_rep = cv
        ciphertext = iv + i2b(cv, NUMBER_OF_BYTES)
        ciphertext_rep = ciphertext
        # print('cv = ', cv)
        # tmp = time.perf_counter()
        decrypted_bytes = dec(k, ciphertext)
        plaintext_rep = decrypted_bytes
        c = np.append(b2i(decrypted_bytes, NUMBER_OF_BYTES), hash_ga)
        c_rep = c
        # print('client time to decrypt: ', time.perf_counter() - tmp)
        # c = [int(c[i]) for i in range(self.pp['n'] + 1)]
        # print('c in Rep = ', c)
        beta = np.inner(self.UH, c) % bls381.curve_order
        # print('beta in Rep = ', beta)
        return (beta)
class RC:
    def __init__(self):
        self.pp = None
        self.sk = None
        self.mffe = None
    def setup(self):
        # Generate pp = {h, vk, sk, M, B}
        # random h
        g = bls381.G1
        x = random.randint(0, bls381.curve_order - 1)
        h = bls381.multiply(g, x)

        # random a, b
        a = bls381.multiply(g, random.randint(0, bls381.curve_order - 1))
        b = bls381.multiply(g, random.randint(0, bls381.curve_order - 1))

        # signing and verifying keys
        self.sk = ecdsa.SigningKey.generate(curve=ecdsa.SECP256k1)
        vk = self.sk.get_verifying_key()

        # model M
        model_dir = "modeldir/sdumla"
        model = densenet(embedding_size=args.embedding_size, pretrained=True)
        # model = torch.nn.DataParallel(model, device_ids=[0]).cuda()
        model = torch.nn.DataParallel(model, device_ids=[0]).cpu()
        checkpoint = torch.load(model_dir + '/model_best.pth.tar', map_location=torch.device("cpu"))
        model.load_state_dict(checkpoint['state_dict'])

        # basis B
        n = 1024
        # d = 0.2540647565201628
        # d = d*1.05
        d = 0.25
        self.pp = {'g': g, 'h': h, 'M': model, 'vk': vk, 'a': a, 'b': b}

        # MFFE
        mffe = MFFE()
        mffe.setup(n, d)
        self.mffe = mffe
    def register_client(self, client):
        # Extract beta
        (client.beta, client.delta, client.w, client.iv) = self.mffe.gen(client.x, client.ga)
        client.comu = bls381.add(client.ga, bls381.multiply(self.pp['h'], client.beta))
        hash_msg = hashlib.sha3_256(p2b(client.comu)).digest()
        client.sigmau = self.sk.sign_deterministic(hash_msg, hashfunc=hashlib.sha3_256)
        # return(client.comu, client.sigmau, client.delta, client.w)
    def register_server(self, server):
        server.hy = bls381.multiply(self.pp['h'], server.gamma)
        hash_msg = hashlib.sha3_256(p2b(server.gy)+p2b(server.hy)).digest()
        server.sigmas = self.sk.sign_deterministic(hash_msg, hashfunc=hashlib.sha3_256)
        # return(server.gy, hy, sigmas)
class Server:
    def __init__(self, gamma):
        self.gamma = gamma
        self.gy = bls381.multiply(bls381.G1, gamma)
        self.hy = None
        self.sigmas = None
    def register(self, rc):
        rc.register_server(self)
class Client:
    def __init__(self, alpha):
        self.alpha = alpha
        self.ga = bls381.multiply(bls381.G1, alpha)
        self.beta = None
        self.x = None
        self.comu = None
        self.sigmau = None
        self.delta = None
        self.w = None
        self.iv = None

        self.e = None
        self.c = None
    def register(self, rc):
        rc.register_client(self)
def mfake(pp, mffe, client, server, image_path):
    # global f1, f2
    global c, e
    t_server = 0
    t_client = 0
    # password creation + PAKE
    tmp_server = time.perf_counter()
    assert pp['vk'].verify(client.sigmau, hashlib.sha3_256(p2b(client.comu)).digest(), hashfunc=hashlib.sha3_256)
    cdhs = bls381.multiply(client.comu, server.gamma)
    Zs = int.from_bytes(hashlib.sha3_256(p2b(cdhs)).digest(), byteorder="little")
    rs = random.randint(0, bls381.curve_order - 1)
    grs = bls381.multiply(pp['g'], rs)
    S = bls381.add(bls381.multiply(pp['a'], Zs), grs)
    t_server += time.perf_counter() - tmp_server



    assert pp['vk'].verify(server.sigmas, hashlib.sha3_256(p2b(server.gy)+p2b(server.hy)).digest(), hashfunc=hashlib.sha3_256)
    model = pp['M']

    # with torch.set_grad_enabled(False):
    #     model.eval()
    #     x = model(client.image, False)
    # client.x =

    tmp_client = time.perf_counter()
    x = getBioX(model, image_path)
    # print('Time to extract features ', time.perf_counter() - tmp_client)
    # print('client time get x: ', time.perf_counter() - tmp_client)
    # print('x in authentication = ', client.x)
    # client.x = f2[i]
    beta = mffe.rep(x, client.alpha, client.delta, client.w, client.iv)
    cdhu = bls381.add(bls381.multiply(server.gy, client.alpha), bls381.multiply(server.hy, beta))
    Zu = int.from_bytes(hashlib.sha3_256(p2b(cdhu)).digest(), byteorder="little")

    ru = random.randint(0, bls381.curve_order - 1)
    gru = bls381.multiply(pp['g'], ru)
    grs_ = bls381.add(S, bls381.neg(bls381.multiply(pp['a'], Zu)))
    ku = bls381.multiply(grs_, ru)
    U = bls381.add(gru, bls381.multiply(pp['b'], Zu))
    Authu = hashlib.sha3_256(p2b(grs_) + p2b(gru) + Zu.to_bytes((Zu.bit_length() + 7) // 8, byteorder='little') + p2b(ku)).digest()
    t_client += time.perf_counter() - tmp_client

    tmp_server = time.perf_counter()
    gru_ = bls381.add(U, bls381.neg(bls381.multiply(pp['b'], Zs)))
    ks = bls381.multiply(gru_, rs)
    Authu_ = hashlib.sha3_256(p2b(grs) + p2b(gru_) + Zs.to_bytes((Zs.bit_length() + 7) // 8, byteorder='little') + p2b(ks)).digest()
    if Authu != Authu_:
        Auths = 0
        acc_server = 0
    else:
        acc_server = 1
        Ks = hashlib.sha3_256(p2b(S) + p2b(server.hy) + Zs.to_bytes((Zs.bit_length() + 7) // 8, byteorder='little') + p2b(U) + p2b(ks))
        Auths = hashlib.sha3_256(p2b(grs) + p2b(gru_) + Zs.to_bytes((Zs.bit_length() + 7) // 8, byteorder='little') + p2b(ks)).digest()
    t_server += time.perf_counter() - tmp_server

    tmp_client = time.perf_counter()
    Auths_ = hashlib.sha3_256(p2b(grs_) + p2b(gru) + Zu.to_bytes((Zu.bit_length() + 7) // 8, byteorder='little') + p2b(ku)).digest()
    if Auths != Auths_:
        acc_client = 0
        # exit()
    else:
        acc_client = 1
        Ku = hashlib.sha3_256(p2b(S) + p2b(server.hy) + Zu.to_bytes((Zu.bit_length() + 7) // 8, byteorder='little') + p2b(U) + p2b(ku))
    t_client += time.perf_counter() - tmp_client
    return(acc_client & acc_server, t_client, t_server)

########################

def test_one_attempt():
    global c_reg, c_rep, e_reg, e_rep, plaintext_reg, plaintext_rep, ciphertext_reg, ciphertext_rep
    global rc, server, client

    # Setup RC with public parameters pp, pp_mffe
    rc = RC()
    rc.setup()

    # Server registration
    server = Server(random.randint(0, bls381.curve_order - 1))
    tmp_server = time.perf_counter()
    server.register(rc)
    t_server_reg = time.perf_counter() - tmp_server

    # Client registration
    alpha = random.randint(0, bls381.curve_order - 1)
    client = Client(alpha)
    tmp_client = time.perf_counter()
    client.x = getBioX(rc.pp['M'], image_path="Data_SDUMLA/Database/106/right/index_3.bmp")
    # client.x = getBioX(rc.pp['M'], image_path="feature_extraction/middle_5.bmp")
    client.register(rc)
    t_client_reg = time.perf_counter() - tmp_client


    # Authentication
    # (succ, t_client_auth, t_server_auth) = mfake(rc.pp, rc.mffe, client, server, image_path="Data_SDUMLA/Database/001/left/index_2.bmp")
    (succ, t_client_auth, t_server_auth) = mfake(rc.pp, rc.mffe, client, server,
                                                 image_path="Data_SDUMLA/Database/106/right/middle_3.bmp")
    # (succ, t_client_auth, t_server_auth) = mfake(rc.pp, rc.mffe, client, server,
    #                                              image_path="feature_extraction/middle_5.bmp")
    if succ:
        print('Mutual authenticated!')
    else:
        print('Failed!')

    # Report timing
    print('Time client registration: ', t_client_reg)
    print('Time client authorization: ', t_client_auth)
    print('Time server registration: ', t_server_reg)
    print('Time server authorization: ', t_server_auth)

    print('c_reg = ', c_reg)
    print('c_rep = ', c_rep)
    print('e_reg = ', e_reg)

    print('e_rep = ', e_rep)
    print('plaintext_reg == ', plaintext_reg)
    print('plaintext_rep == ', plaintext_rep)
    print('ciphertext_reg == ', ciphertext_reg)
    print('ciphertext_rep == ', ciphertext_rep)

    assert (all(e_reg == e_rep))
    assert (ciphertext_reg == ciphertext_rep)
    assert (plaintext_reg == plaintext_rep)
    assert (all(c_reg == c_rep))
def test_mfake_dataset():
    # global c_reg, c_rep, e_reg, e_rep, plaintext_reg, plaintext_rep, ciphertext_reg, ciphertext_rep
    global rc, server, client

    # Setup RC with public parameters pp, pp_mffe
    rc = RC()
    rc.setup()

    # Server registration
    server = Server(random.randint(0, bls381.curve_order - 1))
    tmp_server = time.perf_counter()
    server.register(rc)
    t_server_reg = time.perf_counter() - tmp_server

    # Read csv test files
    df = pd.read_csv("Data_SDUMLA/CSVFiles/sdumla_test_pairs_dis.csv")
    db_path = "Data_SDUMLA/Database/"
    db_size = len(df)
    print(db_size)
    # 0-->23850: impostor, 23851-->28620
    num_test = db_size
    # num_test = 100
    num_genuine_pairs = 0
    num_impostor_pairs = 0
    FA = 0
    FR = 0

    total_server_auth = 0
    total_client_auth = 0
    total_client_reg = 0
    total_ake = 0

    # timing cryptographic operations
    # total_fe = 0

    ## random test
    # for test in range(num_test):
    #     i = random.randint(0, db_size-1)
    # test whole dataset
    for i in range(db_size):
        print('Test ', i)
        # Client registration
        alpha = random.randint(0, bls381.curve_order - 1)
        client = Client(alpha)

        tmp_client = time.perf_counter()
        client.x = getBioX(rc.pp['M'], image_path=os.path.join(db_path, df['idx'][i]))
        # total_fe += time.perf_counter() - tmp_client
        # if (i==99):
        #     print('Average time for FE: ', total_fe/100)
        # client.x = getBioX(rc.pp['M'], image_path="feature_extraction/middle_5.bmp")
        client.register(rc)
        t_client_reg = time.perf_counter() - tmp_client
        total_client_reg += t_client_reg

        tmp_ake = time.perf_counter()
        # Authentication
        # (succ, t_client_auth, t_server_auth) = mfake(rc.pp, rc.mffe, client, server, image_path="Data_SDUMLA/Database/001/left/index_2.bmp")
        (succ, t_client_auth, t_server_auth) = mfake(rc.pp, rc.mffe, client, server,
                                                     image_path=os.path.join(db_path, df['idy'][i]))
        t_ake = time.perf_counter() - tmp_ake
        total_ake += t_ake
        if (df['class'][i]==1.0): # genuine pairs
            num_genuine_pairs += 1
            if succ==0:
                print('Unauthenticated - Test not passed, False rejection!')
                FR += 1
            elif succ==1:
                print("Genuine - Test passed! ")
        elif (df['class'][i]==0.0):
            num_impostor_pairs += 1
            if succ==1:
                print('Unauthenticated - Test not passed, False acceptance!')
                FA += 1
            elif succ==0:
                print("Impostor - Test passed!")
        total_server_auth += t_server_auth
        total_client_auth += t_client_auth

    # Report timing
    print('Average computation time for client registration: ', total_client_reg/num_test)
    print('Average computation time for client authorization: ', total_client_auth/num_test)
    print('Average computation time for server authorization: ', total_server_auth/num_test)
    print('Average computation time for AKE: ', total_ake/num_test)

    # Report FAR, FRR
    if (num_impostor_pairs != 0):
        print('FAA = ', 100*FA/num_impostor_pairs)
    if (num_genuine_pairs != 0):
        print('FRR = ', 100*FR/num_genuine_pairs)
    print('ACC = ', 100*(num_test - FA - FR)/num_test)
def test_mffe(list_n, d):
    time_records = pd.DataFrame(np.zeros((len(list_n), 4)), index=[i for i in range(len(list_n))], columns=['n', 't_setup', 't_gen', 't_rep'])
    mffe = MFFE()
    num_repeats = 100
    # time_records.loc[0] = [0, 0, 0, 0]
    for i in range(len(list_n)):
        total_setup = 0
        total_gen = 0
        total_rep = 0
        for k in range(num_repeats):
            x = np.random.rand(list_n[i], 1)
            a = random.randint(0, bls381.curve_order - 1)
            ga = bls381.multiply(bls381.G1, a)
            t_setup = time.perf_counter()
            mffe.setup(list_n[i], d)
            total_setup += time.perf_counter() - t_setup
            t_gen = time.perf_counter()
            beta, delta, w, iv = mffe.gen(x, ga)
            total_gen += time.perf_counter() - t_gen
            t_rep = time.perf_counter()
            mffe.rep(x, a, delta, w, iv)
            total_rep += time.perf_counter() - t_rep
        # print('n = ', n, ' d = ', d)
        # print('t_setup = ', total_setup/num_repeats)
        # print('t_gen = ', total_gen/num_repeats)
        # print('t_rep = ', total_rep/num_repeats)
        time_records.loc[i] = [list_n[i], total_setup/num_repeats, total_gen/num_repeats, total_rep/num_repeats]
    return time_records
def check_time():
    global B
    global model
    global image
    message = b"Hello, world!"
    key = b"secret-key"
    print('Message: ', message)
    n = 1024
    f = np.random.rand(n)
    # print('f = ', f)
    # print(f.shape)
    # sx_ = pd.Series(index=range(n), data=np.array(f))
    k = 1000
    t_h1 = 0
    t_hmac = 0
    t_sgn = 0
    t_vrf = 0
    t_boxtimes = 0
    t_boxplus = 0
    t_FE = 0
    t_CV = 0
    for i in range(k):
        # Check time of H1, t_h2 = 4t_h1
        t = time.perf_counter()
        sha3_256_hash = hashlib.sha3_256(message).digest()
        t_h1 += time.perf_counter() - t
        # print('t_h1 = ', t_h1)

        # Check time of HMAC
        t = time.perf_counter()
        hmac_sha3_256 = hmac.new(key, message, hashlib.sha3_256).digest()
        t_hmac += time.perf_counter() - t
        # print('t_hmac = ', t_hmac)

        # Check time of ECDSA signature
        # Define the private key
        private_key = ecdsa.SigningKey.generate(curve=ecdsa.SECP256k1)
        public_key = private_key.get_verifying_key()
        t = time.perf_counter()
        hash_message = hashlib.sha3_256(message).digest()
        signature = private_key.sign_deterministic(hash_message, hashfunc=hashlib.sha3_256)
        t_sgn += time.perf_counter() - t
        # print('t_sgn = ', t_sgn)

        t = time.perf_counter()
        assert public_key.verify(signature, hash_message, hashfunc=hashlib.sha3_256)
        t_vrf += time.perf_counter() - t
        # print('t_vrf = ', t_vrf)

        # Check time of t_boxtimes
        y = random.randint(0, bls381.curve_order - 1)
        g = bls381.G1
        t = time.perf_counter()
        gamma = bls381.multiply(g, y)
        t_boxtimes += time.perf_counter() - t
        # print('t_boxtimes = ', t_boxtimes)

        # Check time of t_plus
        x = random.randint(0, bls381.curve_order - 1)
        beta = bls381.multiply(g, x)
        t = time.perf_counter()
        com = bls381.add(gamma, beta)
        t_boxplus += time.perf_counter() - t
        # print('t_boxplus = ', t_boxplus)

        # # Check time of t_CV
        # t = time.perf_counter()
        # # c = closest_vector(n, B, np.matmul(np.linalg.inv(B), np.random.rand(n)))
        # c = closest_vector(n, B, f)
        # t_CV += time.perf_counter() - t
        # # print('t_CV = ', t_CV)
        #
        # # Check time of t_FE
        # t = time.perf_counter()
        # # t_FE += feature_extractor.extract_features("feature_extraction/middle_5.bmp", "feature_extraction/middle_5.out", "modeldir/sdumla")
        # f_image = model(image, False)
        # t_FE += time.perf_counter() - t
        # # print('t_FE = ', t_FE)

        # t_client = t_FE + t_CV + 6*t_h1 + 2*t_vrf + 2*t_hmac + 6*t_boxtimes + 3*t_boxplus
        # t_server = t_vrf + 2*t_sgn + 2*t_hmac + 8*t_boxtimes + 4*t_boxplus

    print('Average t_h1 = ', t_h1/k)
    print('Average t_hmac = ', t_hmac/k)
    print('Average t_sgn = ', t_sgn/k)
    print('Average t_vrf = ', t_vrf/k)
    print('Average t_boxtimes = ', t_boxtimes/k)
    print('Average t_boxplus = ', t_boxplus/k)
    # print('Average t_CV = ', t_CV/k)
    # print('Average t_FE = ', t_FE/k)

#########################

def visualize_mffe_time(time_records):
    # mydpi = 10
    n = time_records['n']
    t_setup = time_records['t_setup']
    t_gen = time_records['t_gen']
    t_rep = time_records['t_rep']

    fig, ax = plt.subplots()
    fig.set_size_inches(8, 5)
    plt.yticks(np.arange(0, 1, 0.1))
    plt.xticks(np.arange(0, 1600, 100))
    # ax.axis([100, 2000, 0, 100])
    # ax.set_xscale('log')
    # ax.set_yscale('log')

    ax.plot(n, t_setup, marker='o', color='green', label='MFFE.Setup')
    ax.plot(n, t_gen, marker='s', color='red', label='MFFE.Gen')
    ax.plot(n, t_rep, marker='*', color='blue', label='MFFE.Rep')
    ax.legend(loc='upper left')
    plt.grid()

    ax.set_xlabel('Dimension of the biometric feature vector', fontsize=10)
    ax.set_ylabel('Time (s)', fontsize=10)
    # plt.savefig('mffe-time.png', dpi=mydpi * 10)
    plt.savefig('mffe-time.png')
    plt.show();
def visualize_ake_comaprarison():
    # dict = {'comm. cost': [392, 432, 344, 488], 'comp. cost': [407.92, 515.66, 407.96, 410.58]}
    dict = {'comm. cost': [147648, 392, 432, 344, 488], 'comp. cost': [151037.44, 407.92, 515.66, 407.96, 410.58]}
    df = pd.DataFrame.from_dict(dict)
    df.index = ['[25]', '[26]', '[22]', 'Our-1', 'Our-2']
    ax = df.plot(kind='bar', secondary_y='comp. cost', rot=0, figsize=(8, 5))
    # plt.yticks(np.arange(0, 1, 0.1))
    # plt.xticks(np.arange(0, 1600, 100))

    # plt.ylabel("Comm. cost in bytes")
    ax.set_ylabel('Comm. cost in bytes')
    ax.right_ax.set_ylabel('Comp. cost in ms')
    # plt.title("Comparison of comm. cost and comp. cost between our scheme and others")
    plt.savefig('mfake-compare3.png')
    plt.show()

    # dict = {'comm. cost': [392, 432, 344, 488], 'comp. cost': [407.92, 515.66, 407.96, 410.58]}
    dict = {'comm. cost': [392, 432, 344, 488], 'comp. cost': [407.92, 515.66, 407.96, 410.58]}
    df = pd.DataFrame.from_dict(dict)
    df.index = ['[26]', '[22]', 'Our-1', 'Our-2']
    ax = df.plot(kind='bar', secondary_y='comp. cost', rot=0, figsize=(8, 5))
    # plt.yticks(np.arange(0, 1, 0.1))
    # plt.xticks(np.arange(0, 1600, 100))

    # plt.ylabel("Comm. cost in bytes")
    ax.set_ylabel('Comm. cost in bytes')
    ax.right_ax.set_ylabel('Comp. cost in ms')
    # plt.title("Comparison of comm. cost and comp. cost between our scheme and others")
    plt.savefig('mfake-compare2.png')
    plt.show()
def visualize_far_frr(list_d, list_far, list_frr):
    fig, ax = plt.subplots()

    ax.plot(list_d, list_far, marker='o', color='g', label='FMR')
    ax.plot(list_d, list_frr, marker='x', color='r', label='FNMR')
    plt.xlabel('d')

    # for i in range(len(list_d)):
    #     a = list_frr[i]
    #     b = list_far[i]
    #     if a == b:
    #         eer = a
    # far_optimum = list_far[np.nanargmin(np.absolute((np.ndarray(list_frr) - np.ndarray(list_far))))]
    # frr_optimum = list_frr[np.nanargmin(np.absolute((np.ndarray(list_frr) - np.ndarray(list_far))))]
    #
    # if (far_optimum == frr_optimum):
    #     eer = far_optimum
    # plt.plot(15, eer, 'ro', label='EER')

    legend = ax.legend(loc='upper left', fontsize='x-large')

    # Put a nicer background color on the legend.
    # legend.get_frame().set_facecolor('C0')
    plt.savefig("d_far_frr.png")
    plt.show()

#########################

def extract_bio_train():
    # Read csv test files
    df = pd.read_csv("Data_SDUMLA/CSVFiles/output_list_train_val.csv")
    # print(df)
    db_path = "Data_SDUMLA/Database/"
    db_size = len(df)
    # Setup RC with public parameters pp, pp_mffe
    rc = RC()
    rc.setup()
    train_features = pd.DataFrame(columns=["class", "bio-feature"])
    for i in range(db_size):
        new_row = {'class': df['class'][i], 'bio-feature': getBioX(rc.pp['M'], image_path=os.path.join(db_path, df['idx'][i]))}
        train_features.loc[-1] = new_row
        train_features.index = train_features.index + 1  # shifting index
        train_features = train_features.sort_index()  # sorting by index
    print(train_features)
    with open(args.outdir + '/train_feature.dat', 'wb') as f:
            pickle.dump(train_features, f)
            print("Dumping features done!")

    if (os.path.exists(args.outdir + '/train_feature.dat')):
        with open(args.outdir + '/train_feature.dat', 'rb') as f:
            train_bio_fvs = pickle.load(f)
        print("Loading features done!")
    print(train_bio_fvs)
def make_training_pairs():
    if (os.path.exists(args.outdir + '/train_pairs.dat')):
        with open(args.outdir + '/train_pairs.dat', 'rb') as f:
            train_pairs = pickle.load(f)
        print("Loading training pairs done!")
        print(train_pairs)
    else:
        if (os.path.exists(args.outdir + '/train_feature.dat')):
            with open(args.outdir + '/train_feature.dat', 'rb') as f:
                train_bio_fvs = pickle.load(f)
            print("Loading features done!")
        train_pairs = pd.DataFrame(columns=["idx1", "idx2", "same-class", "bio-dis"])
        for cls in train_bio_fvs['class'].unique():
            df = train_bio_fvs[train_bio_fvs['class'] == cls]
            for i in range(len(df) - 1):
                for j in range(i + 1, len(df)):
                    same_class = 1
                    idx1 = df.index[i]
                    idx2 = df.index[j]
                    bio_dis = np.linalg.norm(train_bio_fvs['bio-feature'][idx1] - train_bio_fvs['bio-feature'][idx2])
                    new_row = {'idx1': idx1, 'idx2': idx2, 'same-class': same_class, 'bio-dis': bio_dis}
                    train_pairs.loc[-1] = new_row
                    train_pairs.index = train_pairs.index + 1  # shifting index
                    # train_pairs = train_pairs.sort_index()  # sorting by index
        all_classes = train_bio_fvs['class'].unique()
        for k in range(len(all_classes) - 1):
            df = train_bio_fvs[train_bio_fvs['class'] == all_classes[k]]
            i = random.randint(0, len(df) - 1)
            for k_ in range(k + 1, len(all_classes)):
                df_ = train_bio_fvs[train_bio_fvs['class'] == all_classes[k_]]
                j = random.randint(0, len(df_) - 1)
                same_class = 0
                idx1 = df.index[i]
                idx2 = df_.index[j]
                bio_dis = np.linalg.norm(train_bio_fvs['bio-feature'][idx1] - train_bio_fvs['bio-feature'][idx2])
                new_row = {'idx1': idx1, 'idx2': idx2, 'same-class': same_class, 'bio-dis': bio_dis}
                train_pairs.loc[-1] = new_row
                train_pairs.index = train_pairs.index + 1  # shifting index
def extract_bio_test():
    if (os.path.exists(args.outdir + '/test_feature2.dat')):
        with open(args.outdir + '/test_feature2.dat', 'rb') as f:
            test_bio_fvs = pickle.load(f)
        print("Loading testing features done!")
    else:
        # Read csv test files
        df = pd.read_csv("Data_SDUMLA/CSVFiles/output_list_test.csv")
        # print(df)
        db_path = "Data_SDUMLA/Database/"
        db_size = len(df)
        # Setup RC with public parameters pp, pp_mffe
        rc = RC()
        rc.setup()
        test_features = pd.DataFrame(columns=["image-path", "class", "bio-feature"])
        for i in range(db_size):
            new_row = {'image-path': df['idx'][i], 'class': df['class'][i],
                       'bio-feature': getBioX(rc.pp['M'], image_path=os.path.join(db_path, df['idx'][i]))}
            test_features.loc[-1] = new_row
            test_features.index = test_features.index + 1  # shifting index
            test_features = test_features.sort_index()  # sorting by index
        print(test_features)
        with open(args.outdir + '/test_feature2.dat', 'wb') as f:
            pickle.dump(test_features, f)
            print("Dumping testing features done!")

        if (os.path.exists(args.outdir + '/test_feature2.dat')):
            with open(args.outdir + '/test_feature2.dat', 'rb') as f:
                test_bio_fvs = pickle.load(f)
            print("Loading testing features done!")
        print(test_bio_fvs)
def make_testing_pairs():
    if (os.path.exists(args.outdir + '/test_pairs.dat')):
        with open(args.outdir + '/test_pairs.dat', 'rb') as f:
            test_pairs = pickle.load(f)
        print("Loading testing pairs done!")
        print(test_pairs)
    else:
        if (os.path.exists(args.outdir + '/test_feature.dat')):
            with open(args.outdir + '/test_feature.dat', 'rb') as f:
                test_bio_fvs = pickle.load(f)
            print("Loading test features done!")
        test_pairs = pd.DataFrame(columns=["idx1", "idx2", "same-class", "bio-dis"])
        for cls in test_bio_fvs['class'].unique():
            df = test_bio_fvs[test_bio_fvs['class'] == cls]
            for i in range(len(df) - 1):
                for j in range(i + 1, len(df)):
                    same_class = 1
                    idx1 = df.index[i]
                    idx2 = df.index[j]
                    bio_dis = np.linalg.norm(test_bio_fvs['bio-feature'][idx1] - test_bio_fvs['bio-feature'][idx2])
                    new_row = {'idx1': idx1, 'idx2': idx2, 'same-class': same_class, 'bio-dis': bio_dis}
                    test_pairs.loc[-1] = new_row
                    test_pairs.index = test_pairs.index + 1  # shifting index
                    # train_pairs = train_pairs.sort_index()  # sorting by index
        all_classes = test_bio_fvs['class'].unique()
        for k in range(len(all_classes) - 1):
            df = test_bio_fvs[test_bio_fvs['class'] == all_classes[k]]
            i = random.randint(0, len(df) - 1)
            for k_ in range(k + 1, len(all_classes)):
                df_ = test_bio_fvs[test_bio_fvs['class'] == all_classes[k_]]
                j = random.randint(0, len(df_) - 1)
                same_class = 0
                idx1 = df.index[i]
                idx2 = df_.index[j]
                bio_dis = np.linalg.norm(test_bio_fvs['bio-feature'][idx1] - test_bio_fvs['bio-feature'][idx2])
                new_row = {'idx1': idx1, 'idx2': idx2, 'same-class': same_class, 'bio-dis': bio_dis}
                test_pairs.loc[-1] = new_row
                test_pairs.index = test_pairs.index + 1  # shifting index
        print(test_pairs)
        with open(args.outdir + '/test_pairs.dat', 'wb') as f:
            pickle.dump(test_pairs, f)
            print("Dumping testing pairs done!")

        if (os.path.exists(args.outdir + '/test_pairs.dat')):
            with open(args.outdir + '/test_pairs.dat', 'rb') as f:
                test_pairs = pickle.load(f)
            print("Loading testing features done!")
        print(test_pairs)
def make_testing_pairs_2():
    if (os.path.exists(args.outdir + '/test_pairs2.dat')):
        with open(args.outdir + '/test_pairs2.dat', 'rb') as f:
            test_pairs = pickle.load(f)
        print("Loading testing pairs done!")
        print(test_pairs)
    else:
        if (os.path.exists(args.outdir + '/test_feature2.dat')):
            with open(args.outdir + '/test_feature2.dat', 'rb') as f:
                test_bio_fvs = pickle.load(f)
            print("Loading test features done!")
        # Read csv test pair files
        df = pd.read_csv("Data_SDUMLA/CSVFiles/sdumla_test_pairs_dis.csv")
        # print(df)
        # db_path = "Data_SDUMLA/Database/"
        # db_size = len(df)

        test_pairs = pd.DataFrame(columns=["idx1", "idx2", "same-class", "bio-dis"])
        for i in range(len(df)):
            same_class = df['class'][i]
            idx1 = test_bio_fvs[test_bio_fvs['image-path'] == df['idx'][i]].index[0]
            idx2 = test_bio_fvs[test_bio_fvs['image-path'] == df['idy'][i]].index[0]
            bio_dis = np.linalg.norm(test_bio_fvs['bio-feature'][idx1] - test_bio_fvs['bio-feature'][idx2])
            new_row = {'idx1': idx1, 'idx2': idx2, 'same-class': same_class, 'bio-dis': bio_dis}
            test_pairs.loc[-1] = new_row
            test_pairs.index = test_pairs.index + 1  # shifting index
            # train_pairs = train_pairs.sort_index()  # sorting by index
        print(test_pairs)
        with open(args.outdir + '/test_pairs2.dat', 'wb') as f:
            pickle.dump(test_pairs, f)
            print("Dumping testing pairs done!")

        if (os.path.exists(args.outdir + '/test_pairs2.dat')):
            with open(args.outdir + '/test_pairs2.dat', 'rb') as f:
                test_pairs = pickle.load(f)
            print("Loading testing features done!")
        print(test_pairs)

#########################
# Global variables
num_genuine_pairs = 4770    #   4770 (train) / 4770 (test)
num_impostor_pairs = 23850  #   23850 (=4770*5) (train) / 23850 (test)
num_top_genuine_pairs = 100
num_top_impostor_pairs = 100
num_loops = 20
count = -1
list_d = []
list_FAR = []
list_FRR = []
# '/train_feature.dat'
# '/train_pairs.dat'
def find_d(file_features, file_pairs):
    global list_d, list_FAR, list_FRR
    if (os.path.exists(args.outdir + file_features)):
        with open(args.outdir + file_features, 'rb') as f:
            bio_fvs = pickle.load(f)
        print("Loading features done!")
    if (os.path.exists(args.outdir + file_pairs)):
        with open(args.outdir + file_pairs, 'rb') as f:
            pairs = pickle.load(f)
        print("Loading pairs done!")
    top_genuine_pairs = pairs[pairs['same-class'] == 1].sort_values(by='bio-dis', ascending=False)[:num_genuine_pairs]
    top_impostor_pairs = pairs[pairs['same-class'] == 0].sort_values(by='bio-dis')[:num_impostor_pairs]
    chosen_pairs = pd.concat([top_genuine_pairs, top_impostor_pairs])
    tpr, fpr, fnr, fpr_optimum, fnr_optimum, accuracy, threshold = evaluate(chosen_pairs['bio-dis'], chosen_pairs['same-class'])
    print('threshold = ', threshold)
    print('fpr_optimum = ', fpr_optimum)
    print('fnr_optimum = ', fnr_optimum)
    print('accuracy = ', accuracy)
    eps = 0.0001
    chosen_pairs = pd.concat([top_genuine_pairs[:num_top_genuine_pairs], top_impostor_pairs[:num_top_impostor_pairs]])
    return estimate_d(chosen_pairs, bio_fvs, fpr_optimum, fnr_optimum, num_genuine_pairs, num_impostor_pairs, eps, 0, threshold)

def estimate_d(pairs, fvs, fpr_optimum, fnr_optimum, num_genuine_pairs, num_impostor_pairs, eps, l, r, n=1024):
    global count
    global list_d, list_FAR, list_FRR
    print('*' * 100)
    count += 1
    print('count = ', count)
    list_d.append((r-l)/2 + l)
    list_d[0] = 0.254
    print('l = ', l, ', r = ', r, ', d = ', list_d[count])

    FA = []
    FR = []
    B = np.zeros((n, n))
    generate_B_tri(n, list_d[count], B)
    for i in pairs.index:
        diff = fvs['bio-feature'][pairs['idx1'][i]] - fvs['bio-feature'][pairs['idx2'][i]]
        cv = closest_vector(n, B, np.matmul(np.linalg.inv(B), diff).reshape((n,)))
        if (np.count_nonzero(cv) == 0) and (pairs['same-class'][i] == 0):
            FA.append(i)
        elif (np.count_nonzero(cv) > 0) and (pairs['same-class'][i] == 1):
            FR.append(i)

    FAR = 100 * len(FA)/num_impostor_pairs
    FRR = 100 * len(FR)/num_genuine_pairs
    list_FAR.append(FAR)
    list_FRR.append(FRR)
    # ACC = 100 * (num_impostor_pairs + num_genuine_pairs - len(FA) - len(FR))/(num_impostor_pairs + num_genuine_pairs)
    print('FA = ', FA)
    print('FR = ', FR)
    print('FAR = ', FAR)
    print('FRR = ', FRR)
    if count == num_loops:
        return (list_d, list_FAR, list_FRR)

    if (FAR > FRR + math.fabs(fpr_optimum-fnr_optimum) + eps): # decrease d
        r = list_d[count]
        estimate_d(pairs, fvs, fpr_optimum, fnr_optimum, num_genuine_pairs, num_impostor_pairs, eps, l, r, n)

    if (FRR > FAR + math.fabs(fpr_optimum - fnr_optimum) + eps): # increase d
        l = list_d[count]
        estimate_d(pairs, fvs, fpr_optimum, fnr_optimum, num_genuine_pairs, num_impostor_pairs, eps, l, r, n)
    return (list_d, list_FAR, list_FRR)

# def find_d_test():
#     global list_d, list_FAR, list_FRR
#     if (os.path.exists(args.outdir + '/test_feature.dat')):
#         with open(args.outdir + '/test_feature.dat', 'rb') as f:
#             train_bio_fvs = pickle.load(f)
#         print("Loading training features done!")
#     if (os.path.exists(args.outdir + '/test_pairs.dat')):
#         with open(args.outdir + '/test_pairs.dat', 'rb') as f:
#             train_pairs = pickle.load(f)
#         print("Loading traininging pairs done!")
#     top_genuine_pairs = train_pairs[train_pairs['same-class'] == 1].sort_values(by='bio-dis', ascending=False)[
#                         :num_genuine_pairs]
#     top_impostor_pairs = train_pairs[train_pairs['same-class'] == 0].sort_values(by='bio-dis')[:num_impostor_pairs]
#     chosen_pairs = pd.concat([top_genuine_pairs, top_impostor_pairs])
#     tpr, fpr, fnr, fpr_optimum, fnr_optimum, accuracy, threshold = evaluate(chosen_pairs['bio-dis'],
#                                                                             chosen_pairs['same-class'])
#     print('threshold = ', threshold)
#     print('fpr_optimum = ', fpr_optimum)
#     print('fnr_optimum = ', fnr_optimum)
#     print('accuracy = ', accuracy)
#     eps = 0.0001
#     chosen_pairs = pd.concat([top_genuine_pairs[:num_top_genuine_pairs], top_impostor_pairs[:num_top_impostor_pairs]])
#     return estimate_d(chosen_pairs, train_bio_fvs, fpr_optimum, fnr_optimum, num_genuine_pairs, num_impostor_pairs, eps,
#                       0, threshold)
# def find_d_train():
#     global list_d, list_FAR, list_FRR
#     if (os.path.exists(args.outdir + '/train_feature.dat')):
#         with open(args.outdir + '/train_feature.dat', 'rb') as f:
#             train_bio_fvs = pickle.load(f)
#         print("Loading training features done!")
#     if (os.path.exists(args.outdir + '/train_pairs.dat')):
#         with open(args.outdir + '/train_pairs.dat', 'rb') as f:
#             train_pairs = pickle.load(f)
#         print("Loading traininging pairs done!")
#     top_genuine_pairs = train_pairs[train_pairs['same-class'] == 1].sort_values(by='bio-dis', ascending=False)[:num_genuine_pairs]
#     top_impostor_pairs = train_pairs[train_pairs['same-class'] == 0].sort_values(by='bio-dis')[:num_impostor_pairs]
#     chosen_pairs = pd.concat([top_genuine_pairs, top_impostor_pairs])
#     tpr, fpr, fnr, fpr_optimum, fnr_optimum, accuracy, threshold = evaluate(chosen_pairs['bio-dis'], chosen_pairs['same-class'])
#     print('threshold = ', threshold)
#     print('fpr_optimum = ', fpr_optimum)
#     print('fnr_optimum = ', fnr_optimum)
#     print('accuracy = ', accuracy)
#     eps = 0.0001
#     chosen_pairs = pd.concat([top_genuine_pairs[:num_top_genuine_pairs], top_impostor_pairs[:num_top_impostor_pairs]])
#     return estimate_d(chosen_pairs, train_bio_fvs, fpr_optimum, fnr_optimum, num_genuine_pairs, num_impostor_pairs, eps, 0, threshold)
##########################

# def test_performance():
#     # Init time counter
#     t_client_reg = 0
#     t_server_reg = 0
#     t_client_auth = 0
#     t_server_auth = 0
#
#     # Loading test extracted bioX
#     if (os.path.exists(args.outdir + '/test_feature_pairs.dat')):
#         with open(args.outdir + '/test_feature_pairs.dat', 'rb') as f:
#             feature_pairs = pickle.load(f)
#         print("Loading features done!")
#     f1 = np.array([feature for feature_batch in feature_pairs['f1'] for feature in feature_batch])
#     f2 = np.array([feature for feature_batch in feature_pairs['f2'] for feature in feature_batch])
#     num_test_pairs = len(f1)
#     # if (os.path.exists(args.outdir + '/test_outputs.dat')):
#     #     with open(args.outdir + '/test_outputs.dat', 'rb') as f:
#     #         outputs = pickle.load(f)
#     #     print("Loading outputs done!")
#     # print('Max distance: ', np.max(outputs['distances']))
#     # print('Min distance: ', np.min(outputs['distances']))
#     #
#     # # n = len(f1[0])
#     # num_test_pairs = len(f1)
#     # num_genuine_pairs = np.count_nonzero(outputs['labels'])  # = FN_TP
#     # num_impostor_pairs = num_test_pairs - num_genuine_pairs  # = FP_TN
#     # # print('n = ', n)
#     # print('num_test_pairs = ', num_test_pairs)
#     # print('num_genuine_pairs = ', num_genuine_pairs)
#     # print('num_impostor_samples = ', num_impostor_pairs)
#     #
#     # dist_impostor_pairs = pd.Series(index=range(num_impostor_pairs), data=outputs['distances'][0:num_impostor_pairs])
#     # dist_impostor_pairs = dist_impostor_pairs.sort_values(ascending=True)
#     # # print(dist_impostor_pairs[0:10])
#     # # print(dis_impostor_pairs.index[0:10])
#     #
#     # dist_genuine_pairs = pd.Series(index=range(num_impostor_pairs, num_test_pairs),
#     #                                data=outputs['distances'][num_impostor_pairs: num_test_pairs])
#     # dist_genuine_pairs = dist_genuine_pairs.sort_values(ascending=False)
#     # # print(dist_genuine_pairs[0:10])
#
#     # Setup RC, Server, Client
#
#     # Setup RC, generate public parameters pp, pp_mffe
#
#     # Setup RC with public parameters pp, pp_mffe
#     rc = RC()
#     rc.setup()
#
#     # Server registration
#     server = Server(random.randint(0, bls381.curve_order - 1))
#     tmp_server = time.perf_counter()
#     server.register(rc)
#     t_server_reg += time.perf_counter() - tmp_server
#     print('t_reg_server = ', t_server_reg)
#
#     # Test each pair
#     for i in range(num_test_pairs):
#         # Client registration
#         tmp_client = time.perf_counter()
#         client = Client(random.randint(0, bls381.curve_order - 1))
#         getBioX(rc.pp['M'])
#         client.x = f1[i]
#         client.register(rc)
#         t_client_reg += time.perf_counter() - tmp_client
#
#         # Client authentication
#         (succ, tmp_client, tmp_server) = mfake(rc.pp, rc.mffe, client, server, i)
#         if succ:
#             print('Mutual authenticated!')
#         else:
#             print('Failed!')
#         t_client_auth += tmp_client
#         t_server_auth += tmp_server
#
#     # Report timing
#     print('Time client registration: ', t_client_reg / num_test_pairs)
#     print('Time client authorization: ', t_client_auth / num_test_pairs)
#     print('Time server registration: ', t_server_reg)
#     print('Time server authorization: ', t_server_auth / num_test_pairs)


# def auth(pair):
#     global B
#     # hex_string_q = "0x73eda753299d7d483339d80809a1d80553bda402fffe5bfeffffffff00000001"
#     # q = int(hex_string, 16)
#     # print(q)
#     # print(bls381.curve_order)  # q = bls381.curve_order
#
#     # i = random.randint(0, num_test_pairs)
#     # print('Authenticate pair ', i)
#
#     # print("-"*60, "Registration", "-"*60)
#     time_u1 = time.perf_counter()
#     # Hash pwd
#     alphabet = string.ascii_letters + string.digits
#     pwd = ''.join(secrets.choice(alphabet) for i in range(10))
#     # print('pwd = ', pwd)
#     y = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(), byteorder="little") % bls381.curve_order # y \in Zq where q = bls381.curve_order
#     g = bls381.G1
#     gamma = bls381.multiply(g, y)
#     # print('gamma = ', gamma)
#     a = random.randint(0, bls381.curve_order - 1)
#     h = bls381.multiply(g, a)
#     # print('Time registration at user side: ', time.perf_counter() - time_u1, " seconds")
#
#     # time_idp1 = time.perf_counter()
#     c = np.random.randint(2, size=n)
#     # print('c = ', c)
#     b_cstring = ''.join(format(struct.unpack('!I', struct.pack('!f', v))[0], '032b') for v in c)
#     z = int.from_bytes(hashlib.sha3_256(b_cstring.encode('utf-8')).digest(), byteorder='little') % bls381.curve_order
#     com = bls381.add(gamma, bls381.multiply(h, z))
#     # print('com = ', com)
#     # delta = np.matmul(B, f1[i] - c)
#     delta = np.matmul(np.linalg.inv(B), f1[pair]) - c
#     # print('delta = ', delta)
#     # print('Time registration at IDP side: ', time.perf_counter() - time_idp1, " seconds")
#     # print('Registration time: ', time.perf_counter() - time_u1, " seconds")
#
#     # print("-" * 60, "Authentication", "-" * 60)
#     time_u2 = time.perf_counter()
#     r1 = random.randint(0, bls381.curve_order - 1)
#     r2 = random.randint(0, bls381.curve_order - 1)
#     e = random.randint(0, bls381.curve_order - 1)
#     d = bls381.add(bls381.multiply(g, r1), bls381.multiply(h, r2))
#     # print('d = ', d)
#     y_ = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(), byteorder='little') % bls381.curve_order
#     # print('y - y_ = ', y - y_)
#
#     c_ = np.array(closest_vector(n, B, np.matmul(np.linalg.inv(B), f2[pair]) - delta), dtype=int)
#     # print('c_ = ', c_)
#     b_c_string = ''.join(format(struct.unpack('!I', struct.pack('!f', v))[0], '032b') for v in c_)
#     # print('the number of non-zeros in c - c_: ', np.count_nonzero(c - c_))
#     z_ = int.from_bytes(hashlib.sha3_256(b_c_string.encode('utf-8')).digest(), byteorder='little') % bls381.curve_order
#     # print('z - z_ = ', z - z_)
#     u = r1 + e*y_
#     v = r2 + e*z_
#
#     left = bls381.add(bls381.multiply(g, u), bls381.multiply(h, v))
#     right = bls381.add(d, bls381.multiply(com, e))
#     # if bls381.is_inf(bls381.add(left, bls381.neg(right))):
#     if bls381.eq(left, right):
#         # print('Authenticated!')
#         if (labels[pair] == 0):
#             FA.append(pair)
#             print("FA: ", pair)
#     else:
#         # print('Unauthenticated!')
#         if (labels[pair] == 1):
#             FR.append(pair)
#             print("FR: ", pair)
#     # print('Authentication time: ', time.perf_counter() - time_u2, ' seconds')
#     # print('left = ', left)
#     # print('right = ', right)
#
#     return (time.perf_counter() - time_u1, time.perf_counter() - time_u2)

# def test_auth():
#     global f1
#     global f2
#     global labels
#     global n
#     global d
#     global B
#     global num_test_pairs
#     global FA
#     global FR
#     # Load model
#     t = time.perf_counter()
#     # loss_metric = ArcMarginProduct(args.embedding_size, args.num_classes, s=args.scale_rate, m=args.margin)
#     # model = net(embedding_size=args.embedding_size, class_size=args.num_classes, only_embeddings=True,
#     #                 pretrained=True)
#     # to_be_optimized = chain(model.parameters(), loss_metric.parameters())
#     # model = torch.nn.DataParallel(model)
#     # optimizer = torch.optim.SGD(to_be_optimized,
#     #                             lr=args.learning_rate,
#     #                             momentum=0.9,
#     #                             weight_decay=args.weight_decay)
#     # # scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, cooldown=2, verbose=True)
#     # checkpoint = torch.load(args.outdir + '/model_best.pth.tar', map_location=torch.device("cpu"))
#     # args.start_epoch = checkpoint['epoch']
#     # model.load_state_dict(checkpoint['state_dict'])
#     # optimizer.load_state_dict(checkpoint['optimizer'])
#     #
#     # dataloader = get_dataloader(args.database_dir, args.train_dir, args.valid_dir, args.test_dir,
#     #                               args.batch_size, args.num_workers)
#     labels = []
#     f1, f2 = [], []
#     # with torch.set_grad_enabled(False):
#     #     model.eval()
#     #     for batch_idx, (data1, data2, target) in enumerate(dataloader['test']):
#     #         output1 = model(data1, False)
#     #         output2 = model(data2, False)
#     #         # dist = comparer(torch.sign(F.relu(output1)), torch.sign(F.relu(output2)))  # TODO: sign - torch.sign()
#     #         f1.append(output1.data.cpu().numpy())
#     #         f2.append(output2.data.cpu().numpy())
#     #         labels.append(target.data.cpu().numpy())
#     #         # if batch_idx % 50 == 0:
#     #         #     print('Batch-Index -{}'.format(str(batch_idx)))
#
#     # labels = np.array([sublabel for label in labels for sublabel in label])
#     # f1 = np.array([sub_f for f in f1 for sub_f in f])
#     # f2 = np.array([sub_f for f in f2 for sub_f in f])
#
#     if (os.path.exists(args.outdir + '/test_feature_pairs.dat')):
#         with open(args.outdir + '/test_feature_pairs.dat', 'rb') as f:
#             feature_pairs = pickle.load(f)
#         # print("Loading features done!")
#     # print(len(feature_pairs['f1']))
#     f1 = np.array([feature for feature_batch in feature_pairs['f1'] for feature in feature_batch])
#     f2 = np.array([feature for feature_batch in feature_pairs['f2'] for feature in feature_batch])
#
#     time_feature = time.perf_counter() - t
#     if (os.path.exists(args.outdir + '/test_outputs.dat')):
#         with open(args.outdir + '/test_outputs.dat', 'rb') as f:
#             outputs = pickle.load(f)
#         # print("Loading outputs done!")
#     print('Max distance: ', np.max(outputs['distances']))
#     print('Min distance: ', np.min(outputs['distances']))
#     labels = outputs['labels']
#     n = len(f1[0])
#     # num_test_pairs = len(f1)
#     num_test_pairs = 10
#     num_genuine_pairs = np.count_nonzero(labels)  # = FN_TP
#     num_impostor_pairs = num_test_pairs - num_genuine_pairs  # = FP_TN
#     # print('Average time for feature extraction: ', time_feature / num_test_pairs, ' seconds')
#     # print('n = ', n)
#     # print('num_test_pairs = ', num_test_pairs)
#     # print('num_genuine_pairs = ', num_genuine_pairs)
#     # print('num_impostor_samples = ', num_impostor_pairs)
#
#     d = 0.2540647565201628
#     B = np.zeros((n, n))
#     generate_B_tri(n, d, B)
#     FA = []
#     FR = []
#     t_reg_client = 0
#     t_reg_server = 0
#     t_ake_client = 0
#     t_ake_server = 0
#     t_update_client = 0
#     t_update_server = 0
#     for i in range(num_test_pairs):
#         (t1, t2, t3, t4, t5, t6) = mfake(i)
#         t_reg_client += t1
#         t_reg_server += t2
#         t_ake_client += t3
#         t_ake_server += t4
#         t_update_client += t5
#         t_update_server += t6
#
#
#     print('Average time for registration at client: ', t_reg_client / num_test_pairs, ' seconds')
#     print('Average time for registration at server: ', t_reg_server / num_test_pairs, ' seconds')
#     print('Average time for AKE at client: ', t_ake_client / num_test_pairs, ' seconds')
#     print('Average time for AKE at server: ', t_ake_server / num_test_pairs, ' seconds')
#     print('Average time for update at client: ', t_update_client / num_test_pairs, ' seconds')
#     print('Average time for update at server: ', t_update_server / num_test_pairs, ' seconds')
#     # print('FAR = ', 100*len(FA) / num_impostor_pairs)
#     # print('FRR = ', 100*len(FR) / num_genuine_pairs)
#     # print('ACC = ', 100*(num_test_pairs - len(FA) - len(FR)) / num_test_pairs)

# def setup(N, n, p, q):
#     # generate D \in Z_p^{N*N}
#     D = [[None] * N] * N
#     # g = random.randint(1, p-1)
#     # print("g = ", g)
#     for i in range(N):
#         for j in range(N):
#             # r = random.randint(1, q - 1)
#             # print("r = ", r)
#             D[i][j] = random.randint(0, p-1)
#
#     print("D = ", D)
#
#     # generate A \in Z_q^{N*n}
#     A = [[None] * n] * N
#     for i in range(N):
#         for j in range(n):
#             A[i][j] = random.randint(0, q-1)
#
#     hint_c = np.matmul(np.array(D), np.array(A)) % q
#     return (np.array(D), A, hint_c)
#
# def discrete_gaussian_sample(mean, std_dev, cutoff):
#     # Create a list of integers from -cutoff * std_dev to cutoff * std_dev
#     integers = np.arange(-cutoff * std_dev, cutoff * std_dev + 1, dtype=int)
#
#     # Compute the PDF of the discrete Gaussian distribution
#     pdf = np.exp(-(integers - mean) ** 2 / (2 * std_dev ** 2))
#     pdf /= pdf.sum()
#
#     # Generate a random sample
#     return np.random.choice(integers, p=pdf)
#
# def query(i, N, n, p, q, A): # i >= 1
#     i_row = (i-1) // N
#     i_col = (i-1) % N
#
#     u = [0] * N
#     u[i_col] = 1
#     # return (i_row, i_col)
#     s = [None] * n
#     for i in range(n):
#         s[i] = random.randint(0, q-1)
#
#     e = [None] * N
#     delta = math.floor(q/p)
#     for i in range(N):
#         #e[i] = discrete_gaussian_sample(mean=0, std_dev=6.4, cutoff=pow(2, -3))
#         e[i] = random.randint(-delta/4, delta/4)
#     print("e = ", e)
#     qu = (np.matmul(A, np.array(s)) + np.array(e) + delta * np.array(u)) % q
#     print("qu = ", qu)
#     return (i_row, s, qu)

# def answer(qu, D, q):
#     return np.matmul(D, qu) % q

# def recover(i_row, s, hint_c, ans, delta, p, q):
#     d = (ans[i_row] - np.matmul(hint_c[i_row,:], s)) % q
#     m = round(d / delta) % p
#     # m = (d // delta) % p
#     return m

# def pir():
#     N = 10 # 106 users
#     logq = 32   # 25, modulus ciphertext
#     logp = 8      # modulus plaintext
#     p = pow(2, logp)
#     print('p = ', p)
#     q = pow(2, logq)
#     print('q = ', q)
#     delta = math.floor(q/p)
#     print("delta = ", delta)
#     n = 1024 # 1028
#     # e <-- {-1, 1}
#
#     ts = time.perf_counter()
#     D, A, hint_c = setup(N, n, p, q)
#     print('Time setup = ', time.perf_counter() - ts)
#
#     i = 10
#     print("m = ", D[(i - 1) // N, (i - 1) % N])
#
#     tq = time.perf_counter()
#     i_row, s, qu = query(i, N, n, p, q, A)
#     print('Time query = ', time.perf_counter() - tq)
#
#     ta = time.perf_counter()
#     ans = answer(qu, D, q)
#     print('Time answer = ', time.perf_counter() - ta)
#
#     tr = time.perf_counter()
#     m = recover(i_row, s, hint_c, ans, delta, p, q)
#     print('Time recover = ', time.perf_counter() - tr)
#     print("decrypt to m = ", m)
#

#
# def protocol_mfake(pair):
#     global B
#     global model
#     global image
#     print("-"*60, "Registration", "-"*60)
#     # Hash pwd
#     # alphabet = string.ascii_letters + string.digits
#     # pwd = ''.join(secrets.choice(alphabet) for i in range(10))
#     # print('pwd = ', pwd)
#     tmp_client = time.perf_counter()
#     f_image = model(image, False)
#     mffe = MFFE()
#     mffe.setup(n, d, B)
#     pwd = "hongyentran"
#     alpha = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(),
#                        byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     ga = bls381.multiply(mffe.pp['g'], alpha)
#     mffe.gen(x, alpha)
#     t_reg_client = time.perf_counter() - tmp_client
#     # print('Time registration at user side: ', t_reg_client, " seconds")
#
#     tmp_server = time.perf_counter()
#
#     c = np.random.randint(2, size=n)
#     # print('c = ', c)
#     # b_cstring = ''.join(format(struct.unpack('!I', struct.pack('!f', v))[0], '032b') for v in c)
#     # beta = int.from_bytes(hashlib.sha3_256(b_cstring.encode('utf-8')).digest(), byteorder='little') % bls381.curve_order
#     beta = int.from_bytes(hashlib.sha3_256(c.__str__().encode()).digest(), byteorder='little') % bls381.curve_order
#     com = bls381.add(gamma, bls381.multiply(h, beta))
#     # print('com = ', com)
#     # delta = np.matmul(B, f1[i] - c)
#     h2_pass = crypto_primitives.to_digest_n_bits(1024, gamma.__str__().encode())
#     h2_pass_bytes = np.frombuffer(h2_pass, dtype=np.uint8)
#     h2_pass_bits = np.unpackbits(h2_pass_bytes)
#     # print(h2_pass_bits)
#     delta = np.matmul(np.linalg.inv(B), f1[pair]) + h2_pass_bits - c
#     # print('delta = ', delta)
#     # print('Time registration at IDP side: ', time.perf_counter() - time_idp1, " seconds")
#     t_reg_server = time.perf_counter() - tmp_server
#     # print('Registration time at client: ', t_reg_client, ' seconds')
#     # print('Registration time at server: ', t_reg_server, ' seconds')
#
#     # print("-" * 60, "Authentication", "-" * 60)
#     tmp_client = time.perf_counter()
#     ru = random.randint(0, bls381.curve_order - 1)
#     r1 = random.randint(0, bls381.curve_order - 1)
#     r2 = random.randint(0, bls381.curve_order - 1)
#     A = bls381.add(bls381.multiply(g, r1), bls381.multiply(h, r2))
#     t_ake_client = time.perf_counter() - tmp_client
#
#     tmp_server = time.perf_counter()
#     rs = random.randint(0, bls381.curve_order - 1)
#     e = random.randint(0, bls381.curve_order - 1)
#     G = bls381.multiply(g, e)
#     H = bls381.multiply(h, e)
#     C = bls381.add(bls381.multiply(com, e), bls381.multiply(g, rs))
#     ms1 = C.__str__() + G.__str__() + H.__str__()
#     hash_ms1 = hashlib.sha3_256(ms1.encode()).digest()
#     signature = private_key.sign_deterministic(hash_ms1, hashfunc=hashlib.sha3_256)
#     t_ake_server = time.perf_counter() - tmp_server
#
#     tmp_client = time.perf_counter()
#     assert public_key.verify(signature, hash_ms1, hashfunc=hashlib.sha3_256)
#     f_image = model(image, False)
#     alpha_ = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(),
#                            byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     g = bls381.G1
#     gamma_ = bls381.multiply(g, alpha)
#     h2_pass_ = crypto_primitives.to_digest_n_bits(1024, gamma_.__str__().encode())
#     h2_pass_bytes_ = np.frombuffer(h2_pass, dtype=np.uint8)
#     h2_pass_bits_ = np.unpackbits(h2_pass_bytes)
#
#     c_ = np.array(closest_vector(n, B, np.matmul(np.linalg.inv(B), f2[pair]) - delta), dtype=int) + h2_pass_bits_
#     # print('c_ = ', c_)
#     # b_c_string = ''.join(format(struct.unpack('!I', struct.pack('!f', v))[0], '032b') for v in c_)
#     # print('the number of non-zeros in c - c_: ', np.count_nonzero(c - c_))
#     # z_ = int.from_bytes(hashlib.sha3_256(b_c_string.encode('utf-8')).digest(), byteorder='little') % bls381.curve_order
#     beta_ = int.from_bytes(hashlib.sha3_256(c_.__str__().encode()).digest(), byteorder='little') % bls381.curve_order
#     # print('z - z_ = ', z - z_)
#     tmp = bls381.add(bls381.multiply(G, alpha_), bls381.multiply(H, beta_))
#     tt = bls381.add(C, tmp)
#     ku = bls381.multiply(bls381.add(C, bls381.neg(tmp)), ru)
#     ku = hashlib.sha3_256(ku.__str__().encode()).digest()
#     # ku = b"secret-key"
#     mus = int.from_bytes(hmac.new(ku, ms1.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     u = ru + r1 + mus * alpha_
#     v = r2 + mus * beta_
#     mu = u.__str__() + v.__str__() + mus.__str__()
#     t_ake_client = t_ake_client + time.perf_counter() - tmp_client
#
#     tmp_server = time.perf_counter()
#     tmp1 = bls381.add(bls381.multiply(g, u), bls381.multiply(h, v))
#     tmp2 = bls381.add(A, bls381.multiply(com, mus))
#     ks = bls381.multiply(bls381.add(tmp1, bls381.neg(tmp2)), rs)
#     ks = hashlib.sha3_256(ks.__str__().encode()).digest()
#     muu = int.from_bytes(hmac.new(ks, mu.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     mus_correct = int.from_bytes(hmac.new(ks, ms1.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     ms2 = muu.__str__()
#     hash_ms2= hashlib.sha3_256(ms2.encode()).digest()
#     signature = private_key.sign_deterministic(hash_ms2, hashfunc=hashlib.sha3_256)
#     t_ake_server = t_ake_server + time.perf_counter() - tmp_server
#
#     tmp_client = time.perf_counter()
#     assert public_key.verify(signature, hash_ms2, hashfunc=hashlib.sha3_256)
#     muu_correct = int.from_bytes(hmac.new(ku, mu.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     t_ake_client = t_ake_client + time.perf_counter() - tmp_client
#     # print('AKE time at client: ', t_ake_client, ' seconds')
#     # print('AKE time at server: ', t_ake_server, ' seconds')
#
#     # print("-" * 60, "Update", "-" * 60)
#     tmp_client = time.perf_counter()
#     g = bls381.G1
#     pwd = "hongyentran"
#     alpha = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(),
#                            byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     gamma = bls381.multiply(g, alpha)
#     h2_pass = crypto_primitives.to_digest_n_bits(1024, gamma.__str__().encode())
#     h2_pass_bytes = np.frombuffer(h2_pass, dtype=np.uint8)
#     h2_pass_bits = np.unpackbits(h2_pass_bytes)
#
#     pwd_new = "unswcanberra"
#     alpha_new = int.from_bytes(hashlib.sha3_256(pwd_new.encode()).digest(),
#                            byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     gamma_new = bls381.multiply(g, alpha_new)
#     h2_pass_new = crypto_primitives.to_digest_n_bits(1024, gamma_new.__str__().encode())
#     h2_pass_new_bytes = np.frombuffer(h2_pass_new, dtype=np.uint8)
#     h2_pass_new_bits = np.unpackbits(h2_pass_new_bytes)
#     delta_new = delta + h2_pass_new_bits - h2_pass_bits
#     delta_gamma = bls381.add(gamma, bls381.neg(gamma_new))
#     t_update_client = time.perf_counter() - tmp_client
#
#     tmp_server = time.perf_counter()
#     com_new = bls381.add(com, delta_gamma)
#     t_update_server = time.perf_counter() - tmp_server
#     # print('Update time at client: ', t_update_client, ' seconds')
#     # print('Update time at server: ', t_update_server, ' seconds')
#
#
#     return (t_reg_client, t_reg_server, t_ake_client, t_ake_server, t_update_client, t_update_server)

# def mfake(pair):
#     global B
#     global model
#     global image
#     # hex_string_q = "0x73eda753299d7d483339d80809a1d80553bda402fffe5bfeffffffff00000001"
#     # q = int(hex_string, 16)
#     # print(q)
#     # print(bls381.curve_order)  # q = bls381.curve_order
#
#     # i = random.randint(0, num_test_pairs)
#     # print('Authenticate pair ', i)
#
#     # print("-"*60, "Registration", "-"*60)
#     # Hash pwd
#     # alphabet = string.ascii_letters + string.digits
#     # pwd = ''.join(secrets.choice(alphabet) for i in range(10))
#     # print('pwd = ', pwd)
#     tmp_client = time.perf_counter()
#     pwd = "hongyentran"
#     alpha = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(),
#                        byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     g = bls381.G1
#     gamma = bls381.multiply(g, alpha)
#     # print('gamma = ', gamma)
#     a = random.randint(0, bls381.curve_order - 1)
#     h = bls381.multiply(g, a)
#     t_reg_client = time.perf_counter() - tmp_client
#     # print('Time registration at user side: ', t_reg_client, " seconds")
#
#     tmp_server = time.perf_counter()
#     f_image = model(image, False)
#     c = np.random.randint(2, size=n)
#     # print('c = ', c)
#     # b_cstring = ''.join(format(struct.unpack('!I', struct.pack('!f', v))[0], '032b') for v in c)
#     # beta = int.from_bytes(hashlib.sha3_256(b_cstring.encode('utf-8')).digest(), byteorder='little') % bls381.curve_order
#     beta = int.from_bytes(hashlib.sha3_256(c.__str__().encode()).digest(), byteorder='little') % bls381.curve_order
#     com = bls381.add(gamma, bls381.multiply(h, beta))
#     # print('com = ', com)
#     # delta = np.matmul(B, f1[i] - c)
#     h2_pass = crypto_primitives.to_digest_n_bits(1024, gamma.__str__().encode())
#     h2_pass_bytes = np.frombuffer(h2_pass, dtype=np.uint8)
#     h2_pass_bits = np.unpackbits(h2_pass_bytes)
#     # print(h2_pass_bits)
#     delta = np.matmul(np.linalg.inv(B), f1[pair]) + h2_pass_bits - c
#     # print('delta = ', delta)
#     # print('Time registration at IDP side: ', time.perf_counter() - time_idp1, " seconds")
#     t_reg_server = time.perf_counter() - tmp_server
#     # print('Registration time at client: ', t_reg_client, ' seconds')
#     # print('Registration time at server: ', t_reg_server, ' seconds')
#
#     # print("-" * 60, "Authentication", "-" * 60)
#     tmp_client = time.perf_counter()
#     ru = random.randint(0, bls381.curve_order - 1)
#     r1 = random.randint(0, bls381.curve_order - 1)
#     r2 = random.randint(0, bls381.curve_order - 1)
#     A = bls381.add(bls381.multiply(g, r1), bls381.multiply(h, r2))
#     t_ake_client = time.perf_counter() - tmp_client
#
#     tmp_server = time.perf_counter()
#     rs = random.randint(0, bls381.curve_order - 1)
#     e = random.randint(0, bls381.curve_order - 1)
#     G = bls381.multiply(g, e)
#     H = bls381.multiply(h, e)
#     C = bls381.add(bls381.multiply(com, e), bls381.multiply(g, rs))
#     ms1 = C.__str__() + G.__str__() + H.__str__()
#     hash_ms1 = hashlib.sha3_256(ms1.encode()).digest()
#     signature = private_key.sign_deterministic(hash_ms1, hashfunc=hashlib.sha3_256)
#     t_ake_server = time.perf_counter() - tmp_server
#
#     tmp_client = time.perf_counter()
#     assert public_key.verify(signature, hash_ms1, hashfunc=hashlib.sha3_256)
#     f_image = model(image, False)
#     alpha_ = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(),
#                            byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     g = bls381.G1
#     gamma_ = bls381.multiply(g, alpha)
#     h2_pass_ = crypto_primitives.to_digest_n_bits(1024, gamma_.__str__().encode())
#     h2_pass_bytes_ = np.frombuffer(h2_pass, dtype=np.uint8)
#     h2_pass_bits_ = np.unpackbits(h2_pass_bytes)
#
#     c_ = np.array(closest_vector(n, B, np.matmul(np.linalg.inv(B), f2[pair]) - delta), dtype=int) + h2_pass_bits_
#     # print('c_ = ', c_)
#     # b_c_string = ''.join(format(struct.unpack('!I', struct.pack('!f', v))[0], '032b') for v in c_)
#     # print('the number of non-zeros in c - c_: ', np.count_nonzero(c - c_))
#     # z_ = int.from_bytes(hashlib.sha3_256(b_c_string.encode('utf-8')).digest(), byteorder='little') % bls381.curve_order
#     beta_ = int.from_bytes(hashlib.sha3_256(c_.__str__().encode()).digest(), byteorder='little') % bls381.curve_order
#     # print('z - z_ = ', z - z_)
#     tmp = bls381.add(bls381.multiply(G, alpha_), bls381.multiply(H, beta_))
#     tt = bls381.add(C, tmp)
#     ku = bls381.multiply(bls381.add(C, bls381.neg(tmp)), ru)
#     ku = hashlib.sha3_256(ku.__str__().encode()).digest()
#     # ku = b"secret-key"
#     mus = int.from_bytes(hmac.new(ku, ms1.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     u = ru + r1 + mus * alpha_
#     v = r2 + mus * beta_
#     mu = u.__str__() + v.__str__() + mus.__str__()
#     t_ake_client = t_ake_client + time.perf_counter() - tmp_client
#
#     tmp_server = time.perf_counter()
#     tmp1 = bls381.add(bls381.multiply(g, u), bls381.multiply(h, v))
#     tmp2 = bls381.add(A, bls381.multiply(com, mus))
#     ks = bls381.multiply(bls381.add(tmp1, bls381.neg(tmp2)), rs)
#     ks = hashlib.sha3_256(ks.__str__().encode()).digest()
#     muu = int.from_bytes(hmac.new(ks, mu.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     mus_correct = int.from_bytes(hmac.new(ks, ms1.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     ms2 = muu.__str__()
#     hash_ms2= hashlib.sha3_256(ms2.encode()).digest()
#     signature = private_key.sign_deterministic(hash_ms2, hashfunc=hashlib.sha3_256)
#     t_ake_server = t_ake_server + time.perf_counter() - tmp_server
#
#     tmp_client = time.perf_counter()
#     assert public_key.verify(signature, hash_ms2, hashfunc=hashlib.sha3_256)
#     muu_correct = int.from_bytes(hmac.new(ku, mu.encode(), hashlib.sha3_256).digest(), byteorder='little') % bls381.curve_order
#     t_ake_client = t_ake_client + time.perf_counter() - tmp_client
#     # print('AKE time at client: ', t_ake_client, ' seconds')
#     # print('AKE time at server: ', t_ake_server, ' seconds')
#
#     # print("-" * 60, "Update", "-" * 60)
#     tmp_client = time.perf_counter()
#     g = bls381.G1
#     pwd = "hongyentran"
#     alpha = int.from_bytes(hashlib.sha3_256(pwd.encode()).digest(),
#                            byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     gamma = bls381.multiply(g, alpha)
#     h2_pass = crypto_primitives.to_digest_n_bits(1024, gamma.__str__().encode())
#     h2_pass_bytes = np.frombuffer(h2_pass, dtype=np.uint8)
#     h2_pass_bits = np.unpackbits(h2_pass_bytes)
#
#     pwd_new = "unswcanberra"
#     alpha_new = int.from_bytes(hashlib.sha3_256(pwd_new.encode()).digest(),
#                            byteorder="little") % bls381.curve_order  # y \in Zq where q = bls381.curve_order
#     gamma_new = bls381.multiply(g, alpha_new)
#     h2_pass_new = crypto_primitives.to_digest_n_bits(1024, gamma_new.__str__().encode())
#     h2_pass_new_bytes = np.frombuffer(h2_pass_new, dtype=np.uint8)
#     h2_pass_new_bits = np.unpackbits(h2_pass_new_bytes)
#     delta_new = delta + h2_pass_new_bits - h2_pass_bits
#     delta_gamma = bls381.add(gamma, bls381.neg(gamma_new))
#     t_update_client = time.perf_counter() - tmp_client
#
#     tmp_server = time.perf_counter()
#     com_new = bls381.add(com, delta_gamma)
#     t_update_server = time.perf_counter() - tmp_server
#     # print('Update time at client: ', t_update_client, ' seconds')
#     # print('Update time at server: ', t_update_server, ' seconds')
#
#
#     return (t_reg_client, t_reg_server, t_ake_client, t_ake_server, t_update_client, t_update_server)

if __name__ == '__main__':
    test_one_attempt()
    # visualize_ake_comaprarison()
    # d = 0.2540647565201628
    # extract_bio_test()
    #make_testing_pairs_2()
    # find_d('/test_feature2.dat', '/test_pairs2.dat')
    # list_d.sort()
    # list_FAR.sort()
    # list_FRR.sort(reverse=True)
    # print('list_d = ', list_d)
    # print('list_FAR = ', list_FAR)
    # print('list_FRR = ', list_FRR)

    #train_feature, train_pairs (uing my make_pairs for train data): FAR, FRR: 0.08% (d =  0.3130744629743276, FAR = FRR = 0.08385744234800839)
    # list_d = [0.2818358242511749, 0.29945056326687336, 0.3082579327747226, 0.3126616175286472, 0.3129368478257675,
    #           0.3130744629743276, 0.31321207812288776, 0.31376253871712834, 0.3148634599056095, 0.3170653022825718,
    #           0.35229478031396866, 0.4227537363767624, 0.5636716485023499]
    # list_FAR = [0.0041928721174004195, 0.025157232704402517, 0.06708595387840671, 0.07966457023060797,
    #             0.07966457023060797, 0.08385744234800839, 0.0880503144654088, 0.09224318658280922, 0.10482180293501048,
    #             0.11320754716981132, 0.31865828092243187, 0.41509433962264153, 0.4192872117400419]
    # list_FRR = [0.10482180293501048, 0.10482180293501048, 0.08385744234800839, 0.08385744234800839, 0.08385744234800839,
    #             0.08385744234800839, 0.06289308176100629, 0.06289308176100629, 0.06289308176100629, 0.06289308176100629,
    #             0.0, 0.0, 0.0]

    #test_feature, test_pairs (using my make_pairs for test data): d = 0.24903574120253325, FAR = 0.13417190775681342, FRR = 0.14675052410901468
    # list_d = [0.20971430838108063, 0.2359285969287157, 0.24903574120253325, 0.24985493771964684, 0.24988053761080664,
    #           0.24989333755638654, 0.2498997375291765, 0.24990133752237398, 0.24990213751897272, 0.2499025375172721,
    #           0.24990293751557147, 0.24990613750196644, 0.24995733728428604, 0.25005973684892524, 0.25026453597820364,
    #           0.25067413423676044, 0.25231252727098763, 0.255589313339442, 0.2621428854763508, 0.31457146257162094,
    #           0.41942861676216125]
    # list_FAR = [0.0, 0.04612159329140461, 0.129979035639413, 0.13417190775681342, 0.13417190775681342,
    #             0.13417190775681342, 0.13417190775681342, 0.13417190775681342, 0.13417190775681342, 0.13417190775681342,
    #             0.13417190775681342, 0.13417190775681342, 0.13417190775681342, 0.13417190775681342, 0.13417190775681342,
    #             0.13417190775681342, 0.14255765199161424, 0.15932914046121593, 0.1928721174004193, 0.389937106918239,
    #             0.4192872117400419]
    # list_FRR = [1.1111111111111112, 0.48218029350104824, 0.14675052410901468, 0.14675052410901468, 0.14675052410901468,
    #             0.14675052410901468, 0.14675052410901468, 0.14675052410901468, 0.14675052410901468, 0.12578616352201258,
    #             0.12578616352201258, 0.12578616352201258, 0.12578616352201258, 0.12578616352201258, 0.12578616352201258,
    #             0.12578616352201258, 0.10482180293501048, 0.020964360587002098, 0.0, 0.0, 0.0]

    # visualize_far_frr(list_d, list_FAR, list_FRR)


    # far = [0.041928721, 0.046121593, 0.054507338, 0.054507338, 0.096436059]
    # frr = [0.167714885, 0.104821803, 0.041928721, 0.020964361, 0]
    # tpr = [1 - frr[i] for i in range(len(frr))]
    # far_optimum = 0.05450733752620545
    # frr_optimum = 0.041928721174004195
    # # plot_roc(fpr, tpr, figure_name=args.outdir + '/Test_ROC-best.png')
    # plot_DET_with_EER(far, frr, far_optimum, frr_optimum,
    #                   figure_name=args.outdir + '/Test_DET-CV.png')
    # plot_roc(far, tpr, figure_name=args.outdir + '/Test_ROC-CV.png')

    # main()
    # auth()
    # with open(args.outdir + '/test_d_FA_FR.dat', 'rb') as f:
    #     d_fa_fr = pickle.load(f)
    #     print("Loading d_fa_fr done!")
    # fpr = pd.Series(d_fa_fr['FAR'])
    # fpr = fpr.sort_values(ascending=True)
    # print('FAR = ', fpr)
    #
    # fnr = pd.Series(d_fa_fr['FRR'])
    # fnr = fnr.sort_values(ascending=False)
    # print('FRR = ', fnr)
